{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "data_dir = os.getcwd() + '/aclImdb/'\n",
    "#combine into a dict and shuffle and then split if possible\n",
    "#Use random to draw number of contents and delete it after\n",
    "NUM_TRAIN_POS = 12500\n",
    "all_train_targets = np.zeros(shape = (2*NUM_TRAIN_POS,), \n",
    "                         dtype = int, order = 'F')\n",
    "all_train_targets[NUM_TRAIN_POS:] = int(1)\n",
    "all_train_data = []\n",
    "\n",
    "train_neg_dir = data_dir+'train/neg/'\n",
    "for file in os.listdir(train_neg_dir):\n",
    "    file_path = os.path.join(train_neg_dir, file)\n",
    "    text = open(file_path, 'r')\n",
    "    text_list = text.read()\n",
    "    text.close()\n",
    "    all_train_data.append(text_list)    \n",
    "\n",
    "train_pos_dir = data_dir+'train/pos/'\n",
    "for file in os.listdir(train_pos_dir):\n",
    "    file_path = os.path.join(train_pos_dir, file)\n",
    "    text = open(file_path, 'r')\n",
    "    text_list = text.read()\n",
    "    text.close()\n",
    "    all_train_data.append(text_list)    \n",
    "\n",
    "test_targets = np.zeros(shape = (2*NUM_TRAIN_POS,), \n",
    "                         dtype = int, order = 'F')\n",
    "test_targets[NUM_TRAIN_POS:] = int(1)\n",
    "test_data = []\n",
    "\n",
    "test_neg_dir = data_dir+'test/neg/'\n",
    "for file in os.listdir(test_neg_dir):\n",
    "    #print('The current i is {}, the current file is {}'.format(int(i), str(file)))\n",
    "    file_path = os.path.join(test_neg_dir, file)\n",
    "    text = open(file_path, 'r')\n",
    "    text_list = text.read()\n",
    "    text.close()\n",
    "    test_data.append(text_list)    \n",
    "\n",
    "test_pos_dir = data_dir+'test/pos/'\n",
    "for file in os.listdir(test_pos_dir):\n",
    "    file_path = os.path.join(test_pos_dir, file)\n",
    "    text = open(file_path, 'r')\n",
    "    text_list = text.read()\n",
    "    text.close()\n",
    "    test_data.append(text_list)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0,len(test_data)-1)\n",
    "print('The rand is {}\\nAnd the content is\\n{}\\nThe sentiment is {}\\n'.format(i,test_data[i], test_targets[i]))\n",
    "if i <= 12499:\n",
    "    print('The content should be file {}\\n'.format(os.listdir(test_neg_dir)[i]))\n",
    "else:\n",
    "    print('The content should be file {}\\n'.format(os.listdir(test_pos_dir)[i-12500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train = list(zip(all_train_data, all_train_targets))\n",
    "random.shuffle(all_train)\n",
    "all_train_data, all_train_targets = zip(*all_train)\n",
    "TRAIN_SPLIT = 5000\n",
    "val_data = list(all_train_data[:TRAIN_SPLIT])\n",
    "val_targets = np.asarray(all_train_targets[:TRAIN_SPLIT], dtype=int, order = 'F')\n",
    "train_data = list(all_train_data[TRAIN_SPLIT:])\n",
    "train_targets = np.asarray(all_train_targets[TRAIN_SPLIT:],dtype = int, order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(train_targets, open(\"./new_tokens_upper/train_targets.p\", \"wb\"))\n",
    "pkl.dump(val_targets, open(\"./new_tokens_upper/val_targets.p\", \"wb\"))\n",
    "pkl.dump(test_targets, open(\"./new_tokens_upper/test_targets.p\", \"wb\"))\n",
    "pkl.dump(train_data, open(\"./new_tokens_upper/train_data.p\", \"wb\"))\n",
    "pkl.dump(test_data, open(\"./new_tokens_upper/test_data.p\", \"wb\"))\n",
    "pkl.dump(val_data, open(\"./new_tokens_upper/val_data.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "HTML_STR = '<br />'\n",
    "QUOTE_MARK = \"\\'\"\n",
    "punc_list = list(punctuations)\n",
    "punc_list.remove('\\'')#Due to abbreviations, it is actually important to keep the single quote mark\n",
    "\n",
    "def tokenize_lower_no_punc(input):\n",
    "    input_copy = input\n",
    "    input_copy = input_copy.replace(HTML_STR, ' ')\n",
    "    for punc in punc_list:\n",
    "        input_copy = input_copy.replace(punc, ' ')\n",
    "    tokens = tokenizer(input_copy)\n",
    "    return [token.text.lower() for token in tokens if (not token.text.isspace())]\n",
    "\n",
    "def tokenize_no_punc(input):\n",
    "    tokens = tokenizer(input)\n",
    "    return [token.text for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_orig(input):\n",
    "    tokens = tokenizer(input)\n",
    "    return [token.text for token in tokens]\n",
    "\n",
    "#Replace all the punctuations with space, while removing white space when tokenizing, while keeping the upper and lower case\n",
    "def tokenize_no_punc_better(input):\n",
    "    input_copy = input\n",
    "    input_copy = input_copy.replace(HTML_STR, ' ')#Get rid of the HTML string    \n",
    "    for punc in punc_list:\n",
    "        input_copy = input_copy.replace(punc, ' ')\n",
    "    tokens = input_copy.split()\n",
    "    return [token for token in tokens if (not token.isspace())]\n",
    "\n",
    "def n_gram_generator(n, tokens):\n",
    "    n_gram_bag = []\n",
    "    for j in range(len(tokens)):\n",
    "        n_gram_bag.append(tuple(tokens[j:j+n]))\n",
    "        if (j+n) == len(tokens):\n",
    "            break;\n",
    "    return n_gram_bag\n",
    "\n",
    "def tokenize_dataset(n, dataset):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize_no_punc_better(sample)\n",
    "        n_gram_tokens = n_gram_generator(n, tokens)\n",
    "        token_dataset.append(n_gram_tokens)\n",
    "        all_tokens += n_gram_tokens\n",
    "        \n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "#Small test to make sure this works well\n",
    "#test_str = 'I ate an Apple Today.this is good; I\\'m just testing...Hopefully this will work plzzzz'\n",
    "#test_tokens = tokenize_no_punc_better(test_str)\n",
    "#n_gram_generator(2, test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tokenize the train sets with upper cases\n",
    "print(\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(4, train_data)\n",
    "print(\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(4, test_data)\n",
    "print(\"Tokenizing val data\")\n",
    "val_data_tokens,  _ = tokenize_dataset(4, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(train_data_tokens, open(\"./new_tokens_upper/train_data_tokens_4.p\", \"wb\"))\n",
    "pkl.dump(test_data_tokens, open(\"./new_tokens_upper/test_data_tokens_4.p\", \"wb\"))\n",
    "pkl.dump(val_data_tokens, open(\"./new_tokens_upper/val_data_tokens_4.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"./new_tokens_upper/all_train_data_tokens_4.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "for i in range(1,5):\n",
    "    print(\"Currently tokenizing i = {}\".format(int(i)))\n",
    "    print(\"Tokenizing train data\")\n",
    "    train_data_tokens, all_train_tokens = tokenize_dataset(i, train_data)\n",
    "    print(\"Tokenizing test data\")\n",
    "    test_data_tokens, _ = tokenize_dataset(i, test_data)\n",
    "    print(\"Tokenizing val data\")\n",
    "    val_data_tokens,  _ = tokenize_dataset(i, val_data)\n",
    "    \n",
    "    train_data_file_name = \"train_data_tokens_\"\n",
    "    test_data_file_name = \"test_data_tokens_\"\n",
    "    val_data_file_name = \"val_data_tokens_\"\n",
    "    all_train_data_file_name = \"all_train_data_tokens_\"\n",
    "    \n",
    "    pkl.dump(train_data_tokens, open(train_data_file_name + str(i) + \".p\", \"wb\"))\n",
    "    pkl.dump(test_data_tokens, open(test_data_file_name + str(i) + \".p\", \"wb\"))\n",
    "    pkl.dump(val_data_tokens, open(val_data_file_name + str(i) + \".p\", \"wb\"))\n",
    "    pkl.dump(all_train_tokens, open(all_train_data_file_name + str(i) + \".p\", \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload the old data and pick only those with specific number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the old data and pick only n-gram tokens\n",
    "train_data_tokens_lower = pkl.load(open(\"./tokens_lower/train_data_tokens_4.p\", \"rb\"))\n",
    "val_data_tokens_lower = pkl.load(open(\"./tokens_lower/val_data_tokens_4.p\", \"rb\"))\n",
    "test_data_tokens_lower = pkl.load(open(\"./tokens_lower/test_data_tokens_4.p\", \"rb\"))\n",
    "all_train_tokens_lower = pkl.load(open(\"./tokens_lower/all_train_data_tokens_4.p\", \"rb\"))\n",
    "new_train_data_tokens_lower = []\n",
    "for token_list in train_data_tokens_lower:\n",
    "    new_token_list = []\n",
    "    for token in token_list:\n",
    "        if len(token) == 4:\n",
    "            new_token_list.append(token)\n",
    "    new_train_data_tokens_lower.append(new_token_list)\n",
    "    \n",
    "new_val_data_tokens_lower = []\n",
    "for token_list in val_data_tokens_lower:\n",
    "    new_token_list = []\n",
    "    for token in token_list:\n",
    "        if len(token) == 4:\n",
    "            new_token_list.append(token)\n",
    "    new_val_data_tokens_lower.append(new_token_list)\n",
    "\n",
    "new_test_data_tokens_lower = []\n",
    "for token_list in test_data_tokens_lower:\n",
    "    new_token_list = []\n",
    "    for token in token_list:\n",
    "        if len(token) == 4:\n",
    "            new_token_list.append(token)\n",
    "    new_test_data_tokens_lower.append(new_token_list)\n",
    "    \n",
    "new_all_train_tokens_lower = []\n",
    "for token in all_train_tokens_lower:\n",
    "    if len(token) == 4:\n",
    "        new_all_train_tokens_lower.append(token)\n",
    "\n",
    "        \n",
    "pkl.dump(new_train_data_tokens_lower, open('./new_tokens_lower/train_data_tokens_4.p', 'wb'))\n",
    "pkl.dump(new_test_data_tokens_lower, open('./new_tokens_lower/test_data_tokens_4.p', 'wb'))\n",
    "pkl.dump(new_val_data_tokens_lower, open('./new_tokens_lower/val_data_tokens_4.p', 'wb'))\n",
    "pkl.dump(new_all_train_tokens_lower, open('./new_tokens_lower/all_train_data_tokens_4.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some function definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "#Build a vocab return id2token and token2id, which is fed on a vocab size\n",
    "def build_vocab(all_tokens, MAX_VOCAB_SIZE):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>']+id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return id2token, token2id\n",
    "\n",
    "#Token2 id dataset which returns an id dataset\n",
    "def token2id_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "#initialize the MovieReviewDataset\n",
    "MAX_REVIEW_LENGTH = 300\n",
    "class MovieReviewDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert(len(data_list) == len(target_list))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index): #retrieves the data tha corresponds to the key \n",
    "        content = self.data_list[index][:MAX_REVIEW_LENGTH]\n",
    "        label = self.target_list[index]\n",
    "        return [content, len(content), label]\n",
    "          \n",
    "def MovieReview_collate_func(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    for datum in batch:\n",
    "        length_list.append(datum[1])\n",
    "        label_list.append(datum[2])\n",
    "    for datum in batch:\n",
    "        padded_sentence = np.pad(np.array(datum[0]),pad_width=((0,MAX_REVIEW_LENGTH-datum[1])),\n",
    "                                 mode =\"constant\", constant_values = 0)\n",
    "        #Pads after the end with number of max length - datum length\n",
    "        data_list.append(padded_sentence)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)] \n",
    "\n",
    "\n",
    "class BagOfNGrams(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super(BagOfNGrams, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx = 0)\n",
    "        self.linear = nn.Linear(emb_dim,2) # As we only have two categories\n",
    "        \n",
    "    def forward(self, data, length):\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim =1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "        \n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "#Test sets for futher testing on the validations sets\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        #print(len(predicted.eq(labels.view_as(predicted)).numpy()))\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def extract_example(loader,model):\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        pred_array = predicted.eq(labels.view_as(predicted)).numpy()\n",
    "        for i in range(0,len(pred_array)):\n",
    "            if pred_array[i] == 1:\n",
    "                correct.append(str(i))\n",
    "            else:\n",
    "                wrong.append(str(i))\n",
    "            if (len(correct) == 3) & (len(wrong) == 3):\n",
    "                return correct, wrong\n",
    "    return correct, wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning for MAX_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading the training targets\n",
    "train_targets = pkl.load(open('./new_tokens_lower/train_targets.p', 'rb'))\n",
    "test_targets = pkl.load(open('./new_tokens_lower/test_targets.p', 'rb'))\n",
    "val_targets = pkl.load(open('./new_tokens_lower/val_targets.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "train_data_tokens = pkl.load(open('./new_tokens_lower/train_data_tokens_1.p', 'rb'))\n",
    "val_data_tokens = pkl.load(open('./new_tokens_lower/val_data_tokens_1.p', 'rb'))\n",
    "test_data_tokens = pkl.load(open('./new_tokens_lower/test_data_tokens_1.p', 'rb'))\n",
    "all_train_tokens = pkl.load(open('./new_tokens_lower/all_train_data_tokens_1.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_vocab_result = []\n",
    "all_vocab_train_loss = []\n",
    "all_vocab_eval_acc = []\n",
    "\n",
    "for MAX_VOCAB_SIZE in range(20000,90000,20000):\n",
    "    \n",
    "    #Build the vocab\n",
    "    print(\"The current MAX_VOCAB_SIZE IS {}\".format(MAX_VOCAB_SIZE))\n",
    "    id2token, token2id = build_vocab(all_train_tokens, MAX_VOCAB_SIZE);\n",
    "    \n",
    "    ##little test\n",
    "    random_id = random.randint(0, len(id2token)-1)\n",
    "    random_token = id2token[random_id]\n",
    "    print(\"The random token is {}\".format(str(random_token)))\n",
    "    print(\"To test the id it should be {}, and the result is {}\".format(random_id, token2id[random_token]))\n",
    "    ##End of test\n",
    "    \n",
    "    #ID the token datasets\n",
    "    train_data_id = token2id_dataset(train_data_tokens)\n",
    "    val_data_id = token2id_dataset(val_data_tokens)\n",
    "    test_data_id = token2id_dataset(test_data_tokens)\n",
    "\n",
    "    print(\"Test the length of token2id_dataset on train data. Lenght should be {}, it is now {}\".format(len(train_data_tokens), len(train_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on val data. Lenght should be {}, it is now {}\".format(len(val_data_tokens), len(val_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on test data. Lenght should be {}, it is now {}\".format(len(test_data_tokens), len(test_data_id)))\n",
    "\n",
    "    ##Initialize the MovieReviewDataset object and create batches.\n",
    "    BATCH_SIZE = 64\n",
    "    train_dataset = MovieReviewDataset(train_data_id, train_targets)\n",
    "    train_loader = DataLoader(dataset = train_dataset,batch_size = BATCH_SIZE,\n",
    "                          collate_fn = MovieReview_collate_func,\n",
    "                            shuffle = True)\n",
    "\n",
    "    val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "    val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                       collate_fn = MovieReview_collate_func,\n",
    "                       shuffle = True)\n",
    "\n",
    "    test_dataset = MovieReviewDataset(test_data_id,test_targets)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE,\n",
    "                        collate_fn = MovieReview_collate_func,\n",
    "                        shuffle = False)\n",
    "    ##End of initializations\n",
    "    \n",
    "    ##Build the BagOfNGram Models\n",
    "    #Needs further tuning\n",
    "    emb_dim = 100\n",
    "    model = BagOfNGrams(len(id2token), emb_dim)\n",
    "    ##End of building models\n",
    "    \n",
    "    #Parameter settings -> tuning needed later\n",
    "    learning_rate= 0.01\n",
    "    num_epochs = 10\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    #Parameter seting ended\n",
    "\n",
    "    #For each vocab sizes, run through and record\n",
    "    train_loss_val = []\n",
    "    eval_acc_val = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, length, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, length, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            train_loss_val.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i% 100 == 0:\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                eval_acc_val.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "    all_vocab_train_loss.append(train_loss_val)\n",
    "    all_vocab_eval_acc.append(eval_acc_val)\n",
    "        \n",
    "    \n",
    "    val_acc = test_model(val_loader, model)\n",
    "    \n",
    "    curr_vocab_result_list = [val_acc]\n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Val Acc {}\".format(val_acc))\n",
    "    all_vocab_result.append(curr_vocab_result_list)\n",
    "\n",
    "train_loss_file = \"./new_tokens_lower/all_vocab_train_loss_1.p\"\n",
    "eval_acc_file = \"./new_tokens_lower/all_vocab_eval_acc_1.p\"\n",
    "result_file = \"./new_tokens_lower/vocab_result_1.p\"\n",
    "print(\"Currently saving files {}, {}, {}\".format(train_loss_file, eval_acc_file, result_file))\n",
    "\n",
    "pkl.dump(all_vocab_train_loss, open(train_loss_file,\"wb\"))\n",
    "pkl.dump(all_vocab_eval_acc, open(eval_acc_file,\"wb\"))\n",
    "pkl.dump(all_vocab_result, open(result_file,\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_list = [200,400,600,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "all_emb_result = []\n",
    "all_emb_train_loss = []\n",
    "all_emb_eval_acc = []\n",
    "\n",
    "for emb_dim in emb_list:\n",
    "    #Build the vocab\n",
    "    MAX_VOCAB_SIZE = 80000\n",
    "    print(\"The current MAX_VOCAB_SIZE IS {}\".format(MAX_VOCAB_SIZE))\n",
    "    id2token, token2id = build_vocab(all_train_tokens, MAX_VOCAB_SIZE);\n",
    "    \n",
    "    ##little test\n",
    "    random_id = random.randint(0, len(id2token)-1)\n",
    "    random_token = id2token[random_id]\n",
    "    print(\"The random token is {}\".format(str(random_token)))\n",
    "    print(\"To test the id it should be {}, and the result is {}\".format(random_id, token2id[random_token]))\n",
    "    ##End of test\n",
    "    \n",
    "    #ID the token datasets\n",
    "    train_data_id = token2id_dataset(train_data_tokens)\n",
    "    val_data_id = token2id_dataset(val_data_tokens)\n",
    "    test_data_id = token2id_dataset(test_data_tokens)\n",
    "\n",
    "    print(\"Test the length of token2id_dataset on train data. Lenght should be {}, it is now {}\".format(len(train_data_tokens), len(train_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on val data. Lenght should be {}, it is now {}\".format(len(val_data_tokens), len(val_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on test data. Lenght should be {}, it is now {}\".format(len(test_data_tokens), len(test_data_id)))\n",
    "\n",
    "    ##Initialize the MovieReviewDataset object and create batches.\n",
    "    BATCH_SIZE = 64\n",
    "    train_dataset = MovieReviewDataset(train_data_id, train_targets)\n",
    "    train_loader = DataLoader(dataset = train_dataset,batch_size = BATCH_SIZE,\n",
    "                          collate_fn = MovieReview_collate_func,\n",
    "                            shuffle = True)\n",
    "\n",
    "    val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "    val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                       collate_fn = MovieReview_collate_func,\n",
    "                       shuffle = True)\n",
    "\n",
    "    test_dataset = MovieReviewDataset(test_data_id,test_targets)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE,\n",
    "                        collate_fn = MovieReview_collate_func,\n",
    "                        shuffle = False)\n",
    "    ##End of initializations\n",
    "    \n",
    "    ##Build the BagOfNGram Models\n",
    "    #Needs further tuning\n",
    "    print('The current emb_dim is {}'.format(emb_dim))\n",
    "    model = BagOfNGrams(len(id2token), emb_dim)\n",
    "    ##End of building models\n",
    "    \n",
    "    #Parameter settings -> tuning needed later\n",
    "    learning_rate= 0.01\n",
    "    num_epochs = 10\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    #Parameter seting ended\n",
    "\n",
    "    #For each emb sizes, run through and record\n",
    "    train_loss_val = []\n",
    "    eval_acc_val = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, length, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, length, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            train_loss_val.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i% 100 == 0:\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                eval_acc_val.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "    all_emb_train_loss.append(train_loss_val)\n",
    "    all_emb_eval_acc.append(eval_acc_val)\n",
    "        \n",
    "    \n",
    "    val_acc = test_model(val_loader, model)\n",
    "    \n",
    "    curr_emb_result_list = [val_acc]\n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Val Acc {}\".format(val_acc))\n",
    "    all_emb_result.append(curr_emb_result_list)\n",
    "\n",
    "train_loss_file = \"./new_tokens_lower/all_emb_train_loss_1.p\"\n",
    "eval_acc_file = \"./new_tokens_lower/all_emb_eval_acc_1.p\"\n",
    "result_file = \"./new_tokens_lower/emb_result_1.p\"\n",
    "print(\"Currently saving files {}, {}, {}\".format(train_loss_file, eval_acc_file, result_file))\n",
    "\n",
    "pkl.dump(all_emb_train_loss, open(train_loss_file,\"wb\"))\n",
    "pkl.dump(all_emb_eval_acc, open(eval_acc_file,\"wb\"))\n",
    "pkl.dump(all_emb_result, open(result_file,\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Therefore, based on the validation accuracy, I will choose vocab_size = 80000 and emb_dim = 200 for n= 2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_n_result = []\n",
    "all_n_train_loss = []\n",
    "all_n_eval_acc = []\n",
    "\n",
    "for i in range(2,5):    \n",
    "    train_data_file_name = './new_tokens_lower/train_data_tokens_' + str(i) +'.p'\n",
    "    val_data_file_name = './new_tokens_lower/val_data_tokens_'+str(i)+'.p'\n",
    "    test_data_file_name = './new_tokens_lower/test_data_tokens_'+ str(i) + '.p'\n",
    "    all_data_file_name = './new_tokens_lower/all_train_data_tokens_'+ str(i) +'.p'\n",
    "    \n",
    "    print('Currently loading file {}'.format(train_data_file_name))\n",
    "    train_data_tokens = pkl.load(open(train_data_file_name, 'rb'))\n",
    "    print('Currently loading file {}'.format(val_data_file_name))\n",
    "\n",
    "    val_data_tokens = pkl.load(open(val_data_file_name, 'rb'))\n",
    "    print('Currently loading file {}'.format(test_data_file_name))\n",
    "    test_data_tokens = pkl.load(open(test_data_file_name, 'rb'))\n",
    "    print('Currently loading file {}'.format(all_data_file_name))\n",
    "    all_train_tokens = pkl.load(open(all_data_file_name,'rb'))\n",
    "    \n",
    "    MAX_VOCAB_SIZE = 80000\n",
    "    print(\"The current MAX_VOCAB_SIZE IS {}\".format(MAX_VOCAB_SIZE))\n",
    "    id2token, token2id = build_vocab(all_train_tokens, MAX_VOCAB_SIZE);\n",
    "    \n",
    "    ##little test\n",
    "    random_id = random.randint(0, len(id2token)-1)\n",
    "    random_token = id2token[random_id]\n",
    "    print(\"The random token is {}\".format(str(random_token)))\n",
    "    print(\"To test the id it should be {}, and the result is {}\".format(random_id, token2id[random_token]))\n",
    "    ##End of test\n",
    "    \n",
    "    #ID the token datasets\n",
    "    train_data_id = token2id_dataset(train_data_tokens)\n",
    "    val_data_id = token2id_dataset(val_data_tokens)\n",
    "    test_data_id = token2id_dataset(test_data_tokens)\n",
    "\n",
    "    print(\"Test the length of token2id_dataset on train data. Lenght should be {}, it is now {}\".format(len(train_data_tokens), len(train_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on val data. Lenght should be {}, it is now {}\".format(len(val_data_tokens), len(val_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on test data. Lenght should be {}, it is now {}\".format(len(test_data_tokens), len(test_data_id)))\n",
    "\n",
    "    ##Initialize the MovieReviewDataset object and create batches.\n",
    "    BATCH_SIZE = 64\n",
    "    train_dataset = MovieReviewDataset(train_data_id, train_targets)\n",
    "    train_loader = DataLoader(dataset = train_dataset,batch_size = BATCH_SIZE,\n",
    "                          collate_fn = MovieReview_collate_func,\n",
    "                            shuffle = True)\n",
    "\n",
    "    val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "    val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                       collate_fn = MovieReview_collate_func,\n",
    "                       shuffle = True)\n",
    "\n",
    "    test_dataset = MovieReviewDataset(test_data_id,test_targets)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE,\n",
    "                        collate_fn = MovieReview_collate_func,\n",
    "                        shuffle = False)\n",
    "    ##End of initializations\n",
    "    \n",
    "    ##Build the BagOfNGram Models\n",
    "    #Needs further tuning\n",
    "    emb_dim = 200\n",
    "    print('The current emb_dim is {}'.format(emb_dim))\n",
    "    model = BagOfNGrams(len(id2token), emb_dim)\n",
    "    ##End of building models\n",
    "    \n",
    "    #Parameter settings -> tuning needed later\n",
    "    learning_rate= 0.01\n",
    "    num_epochs = 10\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    #Parameter seting ended\n",
    "\n",
    "    #For each vocab sizes, run through and record\n",
    "    train_loss_val = []\n",
    "    eval_acc_val = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, length, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, length, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            train_loss_val.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i% 100 == 0:\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                eval_acc_val.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "    all_n_train_loss.append(train_loss_val)\n",
    "    all_n_eval_acc.append(eval_acc_val)\n",
    "        \n",
    "    \n",
    "    val_acc = test_model(val_loader, model)\n",
    "    \n",
    "    curr_n_result_list = [val_acc]\n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Val Acc {}\".format(val_acc))\n",
    "    all_n_result.append(curr_n_result_list)\n",
    "\n",
    "train_loss_file = \"./new_tokens_lower/all_n_train_loss_1.p\"\n",
    "eval_acc_file = \"./new_tokens_lower/all_n_eval_acc_1.p\"\n",
    "result_file = \"./new_tokens_lower/n_result_1.p\"\n",
    "print(\"Currently saving files {}, {}, {}\".format(train_loss_file, eval_acc_file, result_file))\n",
    "\n",
    "pkl.dump(all_n_train_loss, open(train_loss_file,\"wb\"))\n",
    "pkl.dump(all_n_eval_acc, open(eval_acc_file,\"wb\"))\n",
    "pkl.dump(all_n_result, open(result_file,\"wb\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the previous rounds, I will use n = 2, max_vocab = 80000, and embedded dimension = 200 to do the following tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate_list = [0.001, 0.003, 0.01, 0.03, 0.1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "train_data_tokens = pkl.load(open('./new_tokens_lower/train_data_tokens_2.p', 'rb'))\n",
    "val_data_tokens = pkl.load(open('./new_tokens_lower/val_data_tokens_2.p', 'rb'))\n",
    "test_data_tokens = pkl.load(open('./new_tokens_lower/test_data_tokens_2.p', 'rb'))\n",
    "all_train_tokens = pkl.load(open('./new_tokens_lower/all_train_data_tokens_2.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_lr_result = []\n",
    "all_lr_train_loss = []\n",
    "all_lr_eval_acc = []\n",
    "\n",
    "for learning_rate in learning_rate_list:\n",
    "    #print('The current learning rate is {}'.format(learning_rate))\n",
    "    #Build the vocab\n",
    "    MAX_VOCAB_SIZE = 80000\n",
    "    print(\"The current MAX_VOCAB_SIZE IS {}\".format(MAX_VOCAB_SIZE))\n",
    "    id2token, token2id = build_vocab(all_train_tokens, MAX_VOCAB_SIZE);\n",
    "    \n",
    "    ##little test\n",
    "    random_id = random.randint(0, len(id2token)-1)\n",
    "    random_token = id2token[random_id]\n",
    "    print(\"The random token is {}\".format(str(random_token)))\n",
    "    print(\"To test the id it should be {}, and the result is {}\".format(random_id, token2id[random_token]))\n",
    "    ##End of test\n",
    "    \n",
    "    #ID the token datasets\n",
    "    train_data_id = token2id_dataset(train_data_tokens)\n",
    "    val_data_id = token2id_dataset(val_data_tokens)\n",
    "    test_data_id = token2id_dataset(test_data_tokens)\n",
    "\n",
    "    print(\"Test the length of token2id_dataset on train data. Lenght should be {}, it is now {}\".format(len(train_data_tokens), len(train_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on val data. Lenght should be {}, it is now {}\".format(len(val_data_tokens), len(val_data_id)))\n",
    "    print(\"Test the length of token2id_dataset on test data. Lenght should be {}, it is now {}\".format(len(test_data_tokens), len(test_data_id)))\n",
    "\n",
    "    ##Initialize the MovieReviewDataset object and create batches.\n",
    "    BATCH_SIZE = 64\n",
    "    train_dataset = MovieReviewDataset(train_data_id, train_targets)\n",
    "    train_loader = DataLoader(dataset = train_dataset,batch_size = BATCH_SIZE,\n",
    "                          collate_fn = MovieReview_collate_func,\n",
    "                            shuffle = True)\n",
    "\n",
    "    val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "    val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                       collate_fn = MovieReview_collate_func,\n",
    "                       shuffle = True)\n",
    "\n",
    "    test_dataset = MovieReviewDataset(test_data_id,test_targets)\n",
    "    test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE,\n",
    "                        collate_fn = MovieReview_collate_func,\n",
    "                        shuffle = False)\n",
    "    ##End of initializations\n",
    "    \n",
    "    ##Build the BagOfNGram Models\n",
    "    #Needs further tuning\n",
    "    emb_dim = 200\n",
    "    print('The current emb_dim is {}'.format(emb_dim))\n",
    "    model = BagOfNGrams(len(id2token), emb_dim)\n",
    "    ##End of building models\n",
    "    \n",
    "    #Parameter settings -> tuning needed later\n",
    "    #learning_rate= 0.01\n",
    "    num_epochs = 10\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    #Parameter seting ended\n",
    "    print('The learning rate here is {}'.format(learning_rate))\n",
    "    #For each learning rate, run through and record\n",
    "    train_loss_val = []\n",
    "    eval_acc_val = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data, length, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, length, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            train_loss_val.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i% 100 == 0:\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                eval_acc_val.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "    all_lr_train_loss.append(train_loss_val)\n",
    "    all_lr_eval_acc.append(eval_acc_val)\n",
    "        \n",
    "    \n",
    "    val_acc = test_model(val_loader, model)\n",
    "    \n",
    "    curr_lr_result_list = [val_acc]\n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Val Acc {}\".format(val_acc))\n",
    "    all_lr_result.append(curr_lr_result_list)\n",
    "\n",
    "train_loss_file = \"./new_tokens_lower/all_lr_train_loss.p\"\n",
    "eval_acc_file = \"./new_tokens_lower/all_lr_eval_acc.p\"\n",
    "result_file = \"./new_tokens_lower/lr_result.p\"\n",
    "print(\"Currently saving files {}, {}, {}\".format(train_loss_file, eval_acc_file, result_file))\n",
    "\n",
    "pkl.dump(all_lr_train_loss, open(train_loss_file,\"wb\"))\n",
    "pkl.dump(all_lr_eval_acc, open(eval_acc_file,\"wb\"))\n",
    "pkl.dump(all_lr_result, open(result_file,\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "train_data_tokens = pkl.load(open('./new_tokens_lower/train_data_tokens_2.p', 'rb'))\n",
    "val_data_tokens = pkl.load(open('./new_tokens_lower/val_data_tokens_2.p', 'rb'))\n",
    "test_data_tokens = pkl.load(open('./new_tokens_lower/test_data_tokens_2.p', 'rb'))\n",
    "all_train_tokens = pkl.load(open('./new_tokens_lower/all_train_data_tokens_2.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build the vocab\n",
    "MAX_VOCAB_SIZE = 80000\n",
    "print(\"The current MAX_VOCAB_SIZE IS {}\".format(MAX_VOCAB_SIZE))\n",
    "id2token, token2id = build_vocab(all_train_tokens, MAX_VOCAB_SIZE);\n",
    "\n",
    "##little test\n",
    "random_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_id]\n",
    "print(\"The random token is {}\".format(str(random_token)))\n",
    "print(\"To test the id it should be {}, and the result is {}\".format(random_id, token2id[random_token]))\n",
    "##End of test\n",
    "\n",
    "#ID the token datasets\n",
    "train_data_id = token2id_dataset(train_data_tokens)\n",
    "val_data_id = token2id_dataset(val_data_tokens)\n",
    "test_data_id = token2id_dataset(test_data_tokens)\n",
    "\n",
    "print(\"Test the length of token2id_dataset on train data. Lenght should be {}, it is now {}\".format(len(train_data_tokens), len(train_data_id)))\n",
    "print(\"Test the length of token2id_dataset on val data. Lenght should be {}, it is now {}\".format(len(val_data_tokens), len(val_data_id)))\n",
    "print(\"Test the length of token2id_dataset on test data. Lenght should be {}, it is now {}\".format(len(test_data_tokens), len(test_data_id)))\n",
    "\n",
    "##Initialize the MovieReviewDataset object and create batches.\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = MovieReviewDataset(train_data_id, train_targets)\n",
    "train_loader = DataLoader(dataset = train_dataset,batch_size = BATCH_SIZE,\n",
    "                      collate_fn = MovieReview_collate_func,\n",
    "                        shuffle = True)\n",
    "\n",
    "val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                   collate_fn = MovieReview_collate_func,\n",
    "                   shuffle = True)\n",
    "\n",
    "test_dataset = MovieReviewDataset(test_data_id,test_targets)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE,\n",
    "                    collate_fn = MovieReview_collate_func,\n",
    "                    shuffle = False)\n",
    "##End of initializations\n",
    "\n",
    "##Build the BagOfNGram Models\n",
    "#Needs further tuning\n",
    "emb_dim = 200\n",
    "print('The current emb_dim is {}'.format(emb_dim))\n",
    "model = BagOfNGrams(len(id2token), emb_dim)\n",
    "##End of building models\n",
    "\n",
    "#Parameter settings -> tuning needed later\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#Parameter seting ended\n",
    "print('The learning rate here is {}'.format(learning_rate))\n",
    "#For each learning rate, run through and record\n",
    "train_loss_val = []\n",
    "eval_acc_val = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, length, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, length, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        train_loss_val.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i > 0 and i% 100 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            eval_acc_val.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                   epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "#all_lr_train_loss.append(train_loss_val)\n",
    "#all_lr_eval_acc.append(eval_acc_val)\n",
    "\n",
    "\n",
    "val_acc = test_model(val_loader, model)\n",
    "\n",
    "curr_SGD_result_list = [val_acc]\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(val_acc))\n",
    "#all_lr_test_result.append(curr_lr_result_list)\n",
    "\n",
    "train_loss_file = \"./new_tokens_lower/SGD_train_loss.p\"\n",
    "eval_acc_file = \"./new_tokens_lower/SGD_eval_acc.p\"\n",
    "result_file = \"./new_tokens_lower/SGD_result.p\"\n",
    "#print(\"Currently saving files {}, {}, {}\".format(train_loss_file, eval_acc_file, result_file))\n",
    "\n",
    "pkl.dump(train_loss_val, open(train_loss_file,\"wb\"))\n",
    "pkl.dump(eval_acc_val, open(eval_acc_file,\"wb\"))\n",
    "pkl.dump(curr_SGD_result_list, open(result_file,\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Linear Annealing of learning rate\n",
    "#Build the vocab\n",
    "MAX_VOCAB_SIZE = 80000\n",
    "print(\"The current MAX_VOCAB_SIZE IS {}\".format(MAX_VOCAB_SIZE))\n",
    "id2token, token2id = build_vocab(all_train_tokens, MAX_VOCAB_SIZE);\n",
    "\n",
    "##little test\n",
    "random_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_id]\n",
    "print(\"The random token is {}\".format(str(random_token)))\n",
    "print(\"To test the id it should be {}, and the result is {}\".format(random_id, token2id[random_token]))\n",
    "##End of test\n",
    "\n",
    "#ID the token datasets\n",
    "train_data_id = token2id_dataset(train_data_tokens)\n",
    "val_data_id = token2id_dataset(val_data_tokens)\n",
    "test_data_id = token2id_dataset(test_data_tokens)\n",
    "\n",
    "print(\"Test the length of token2id_dataset on train data. Lenght should be {}, it is now {}\".format(len(train_data_tokens), len(train_data_id)))\n",
    "print(\"Test the length of token2id_dataset on val data. Lenght should be {}, it is now {}\".format(len(val_data_tokens), len(val_data_id)))\n",
    "print(\"Test the length of token2id_dataset on test data. Lenght should be {}, it is now {}\".format(len(test_data_tokens), len(test_data_id)))\n",
    "\n",
    "##Initialize the MovieReviewDataset object and create batches.\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = MovieReviewDataset(train_data_id, train_targets)\n",
    "train_loader = DataLoader(dataset = train_dataset,batch_size = BATCH_SIZE,\n",
    "                      collate_fn = MovieReview_collate_func,\n",
    "                        shuffle = True)\n",
    "\n",
    "val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                   collate_fn = MovieReview_collate_func,\n",
    "                   shuffle = True)\n",
    "\n",
    "test_dataset = MovieReviewDataset(test_data_id,test_targets)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE,\n",
    "                    collate_fn = MovieReview_collate_func,\n",
    "                    shuffle = False)\n",
    "##End of initializations\n",
    "\n",
    "##Build the BagOfNGram Models\n",
    "#Needs further tuning\n",
    "emb_dim = 200\n",
    "print('The current emb_dim is {}'.format(emb_dim))\n",
    "model = BagOfNGrams(len(id2token), emb_dim)\n",
    "##End of building models\n",
    "\n",
    "#Parameter settings -> tuning needed later\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#Linear annealing\n",
    "lambda_1 = lambda epoch: 0.9 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda_1)\n",
    "\n",
    "#Parameter seting ended\n",
    "print('The learning rate here is {}'.format(learning_rate))\n",
    "#For each learning rate, run through and record\n",
    "train_loss_val = []\n",
    "eval_acc_val = []\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, length, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, length, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        train_loss_val.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i > 0 and i% 100 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            eval_acc_val.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                   epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "#all_lr_train_loss.append(train_loss_val)\n",
    "#all_lr_eval_acc.append(eval_acc_val)\n",
    "\n",
    "\n",
    "val_acc = test_model(val_loader, model)\n",
    "\n",
    "curr_la_result_list = [val_acc]\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(val_acc))\n",
    "#all_lr_test_result.append(curr_lr_result_list)\n",
    "\n",
    "train_loss_file = \"./new_tokens_lower/la_train_loss.p\"\n",
    "eval_acc_file = \"./new_tokens_lower/la_eval_acc.p\"\n",
    "result_file = \"./new_tokens_lower/la_result.p\"\n",
    "#print(\"Currently saving files {}, {}, {}\".format(train_loss_file, eval_acc_file, result_file))\n",
    "\n",
    "pkl.dump(train_loss_val, open(train_loss_file,\"wb\"))\n",
    "pkl.dump(eval_acc_val, open(eval_acc_file,\"wb\"))\n",
    "pkl.dump(curr_la_result_list, open(result_file,\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "train_data_tokens = pkl.load(open('./new_tokens_lower/train_data_tokens_2.p', 'rb'))\n",
    "val_data_tokens = pkl.load(open('./new_tokens_lower/val_data_tokens_2.p', 'rb'))\n",
    "test_data_tokens = pkl.load(open('./new_tokens_lower/test_data_tokens_2.p', 'rb'))\n",
    "all_train_tokens = pkl.load(open('./new_tokens_lower/all_train_data_tokens_2.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_targets = pkl.load(open('./new_tokens_lower/train_targets.p', 'rb'))\n",
    "test_targets = pkl.load(open('./new_tokens_lower/test_targets.p', 'rb'))\n",
    "val_targets = pkl.load(open('./new_tokens_lower/val_targets.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current MAX_VOCAB_SIZE IS 80000\n",
      "The random token is ('2', 'that')\n",
      "To test the id it should be 44758, and the result is 44758\n",
      "Test the length of token2id_dataset on train data. Lenght should be 20000, it is now 20000\n",
      "Test the length of token2id_dataset on val data. Lenght should be 5000, it is now 5000\n",
      "Test the length of token2id_dataset on test data. Lenght should be 25000, it is now 25000\n",
      "The current emb_dim is 200\n",
      "The learning rate here is 0.001\n",
      "Epoch: [1/10], Step: [101/313], Validation Acc: 56.92\n",
      "Epoch: [1/10], Step: [201/313], Validation Acc: 66.88\n",
      "Epoch: [1/10], Step: [301/313], Validation Acc: 75.78\n",
      "Epoch: [2/10], Step: [101/313], Validation Acc: 82.16\n",
      "Epoch: [2/10], Step: [201/313], Validation Acc: 83.84\n",
      "Epoch: [2/10], Step: [301/313], Validation Acc: 85.18\n",
      "Epoch: [3/10], Step: [101/313], Validation Acc: 85.78\n",
      "Epoch: [3/10], Step: [201/313], Validation Acc: 86.72\n",
      "Epoch: [3/10], Step: [301/313], Validation Acc: 86.92\n",
      "Epoch: [4/10], Step: [101/313], Validation Acc: 87.32\n",
      "Epoch: [4/10], Step: [201/313], Validation Acc: 87.54\n",
      "Epoch: [4/10], Step: [301/313], Validation Acc: 87.62\n",
      "Epoch: [5/10], Step: [101/313], Validation Acc: 87.82\n",
      "Epoch: [5/10], Step: [201/313], Validation Acc: 88.04\n",
      "Epoch: [5/10], Step: [301/313], Validation Acc: 88.04\n",
      "Epoch: [6/10], Step: [101/313], Validation Acc: 88.2\n",
      "Epoch: [6/10], Step: [201/313], Validation Acc: 88.16\n",
      "Epoch: [6/10], Step: [301/313], Validation Acc: 88.36\n",
      "Epoch: [7/10], Step: [101/313], Validation Acc: 88.54\n",
      "Epoch: [7/10], Step: [201/313], Validation Acc: 88.14\n",
      "Epoch: [7/10], Step: [301/313], Validation Acc: 88.44\n",
      "Epoch: [8/10], Step: [101/313], Validation Acc: 88.48\n",
      "Epoch: [8/10], Step: [201/313], Validation Acc: 88.64\n",
      "Epoch: [8/10], Step: [301/313], Validation Acc: 88.6\n",
      "Epoch: [9/10], Step: [101/313], Validation Acc: 88.48\n",
      "Epoch: [9/10], Step: [201/313], Validation Acc: 88.54\n",
      "Epoch: [9/10], Step: [301/313], Validation Acc: 88.4\n",
      "Epoch: [10/10], Step: [101/313], Validation Acc: 88.54\n",
      "Epoch: [10/10], Step: [201/313], Validation Acc: 88.6\n",
      "Epoch: [10/10], Step: [301/313], Validation Acc: 88.56\n"
     ]
    }
   ],
   "source": [
    "#Final Model Train to get the test results\n",
    "#Build the vocab\n",
    "MAX_VOCAB_SIZE = 80000\n",
    "print(\"The current MAX_VOCAB_SIZE IS {}\".format(MAX_VOCAB_SIZE))\n",
    "id2token, token2id = build_vocab(all_train_tokens, MAX_VOCAB_SIZE);\n",
    "\n",
    "##little test\n",
    "random_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_id]\n",
    "print(\"The random token is {}\".format(str(random_token)))\n",
    "print(\"To test the id it should be {}, and the result is {}\".format(random_id, token2id[random_token]))\n",
    "##End of test\n",
    "\n",
    "#ID the token datasets\n",
    "train_data_id = token2id_dataset(train_data_tokens)\n",
    "val_data_id = token2id_dataset(val_data_tokens)\n",
    "test_data_id = token2id_dataset(test_data_tokens)\n",
    "\n",
    "print(\"Test the length of token2id_dataset on train data. Lenght should be {}, it is now {}\".format(len(train_data_tokens), len(train_data_id)))\n",
    "print(\"Test the length of token2id_dataset on val data. Lenght should be {}, it is now {}\".format(len(val_data_tokens), len(val_data_id)))\n",
    "print(\"Test the length of token2id_dataset on test data. Lenght should be {}, it is now {}\".format(len(test_data_tokens), len(test_data_id)))\n",
    "\n",
    "##Initialize the MovieReviewDataset object and create batches.\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = MovieReviewDataset(train_data_id, train_targets)\n",
    "train_loader = DataLoader(dataset = train_dataset,batch_size = BATCH_SIZE,\n",
    "                      collate_fn = MovieReview_collate_func,\n",
    "                        shuffle = True)\n",
    "\n",
    "val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                   collate_fn = MovieReview_collate_func,\n",
    "                   shuffle = True)\n",
    "\n",
    "test_dataset = MovieReviewDataset(test_data_id,test_targets)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE,\n",
    "                    collate_fn = MovieReview_collate_func,\n",
    "                    shuffle = False)\n",
    "##End of initializations\n",
    "\n",
    "##Build the BagOfNGram Models\n",
    "#Needs further tuning\n",
    "emb_dim = 200\n",
    "print('The current emb_dim is {}'.format(emb_dim))\n",
    "model = BagOfNGrams(len(id2token), emb_dim)\n",
    "##End of building models\n",
    "\n",
    "#Parameter settings \n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "#Linear annealing\n",
    "lambda_1 = lambda epoch: 0.9 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda_1)\n",
    "\n",
    "#Parameter seting ended\n",
    "print('The learning rate here is {}'.format(learning_rate))\n",
    "#For each learning rate, run through and record\n",
    "train_loss_val = []\n",
    "eval_acc_val = []\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (data, length, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, length, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        train_loss_val.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i > 0 and i% 100 == 0:\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            eval_acc_val.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                   epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "#all_lr_train_loss.append(train_loss_val)\n",
    "#all_lr_eval_acc.append(eval_acc_val)\n",
    "\n",
    "\n",
    "val_acc = test_model(val_loader, model)\n",
    "test_acc = test_model(test_loader, model)\n",
    "\n",
    "curr_result_list = [val_acc, test_acc]\n",
    "#print (\"After training for {} epochs\".format(num_epochs))\n",
    "#print (\"Val Acc {}\".format(val_acc))\n",
    "#print (\"Test Acc {}\".format(test_acc))\n",
    "#all_lr_test_result.append(curr_lr_result_list)\n",
    "\n",
    "#train_loss_file = \"./new_tokens_upper/train_loss.p\"\n",
    "#eval_acc_file = \"./new_tokens_upper/eval_acc.p\"\n",
    "#test_file = \"./new_tokens_upper/test_result.p\"\n",
    "#print(\"Currently saving files {}, {}, {}\".format(train_loss_file, eval_acc_file, test_file))\n",
    "\n",
    "#pkl.dump(train_loss_val, open(train_loss_file,\"wb\"))\n",
    "#pkl.dump(eval_acc_val, open(eval_acc_file,\"wb\"))\n",
    "#pkl.dump(curr_la_result_list, open(test_file,\"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dataset = MovieReviewDataset(val_data_id, val_targets)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE,\n",
    "                   collate_fn = MovieReview_collate_func,\n",
    "                   shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Hengyu/Desktop/MasterWork/1011'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_file = pkl.load(open('./HW1/new_tokens_lower/test_lr_result.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84.6, 83.452]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_file[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations and graph plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_plot_file = pkl.load(open('./new_tokens_lower/all_vocab_train_loss_1.p', 'rb'))\n",
    "val_plot_file = pkl.load(open('./new_tokens_lower/all_vocab_eval_acc_1.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = [20000,40000,60000,80000]\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 4, figsize = (24,8))\n",
    "for i in range(0,4):\n",
    "    ax[0,i].plot(train_plot_file[i])\n",
    "    ax[1,i].plot(val_plot_file[i])\n",
    "    ax[0,i].set_xlabel(\"Training data size\")\n",
    "    ax[1,i].set_xlabel(\"Epochs\")\n",
    "    ax[0,i].set_ylabel(\"Training loss\")\n",
    "    ax[1,i].set_ylabel(\"Evaluation accuracy\")\n",
    "    ax[0,i].set_title(\"Vocab Size: {}\".format(vocab_list[i]))\n",
    "\n",
    "fig.savefig('./new_tokens_lower/vocab_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_plot_file = pkl.load(open('./new_tokens_lower/all_lr_train_loss.p', 'rb'))\n",
    "val_plot_file = pkl.load(open('./new_tokens_lower/all_lr_eval_acc.p', 'rb'))\n",
    "extra_train_file = pkl.load(open('./new_tokens_upper/train_loss.p', 'rb'))\n",
    "extra_val_file = pkl.load(open('./new_tokens_upper/eval_acc.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizer_list = ['Constant', 'Adam']\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (10,5))\n",
    "#for i in range(1,2):\n",
    "#ax[0,0].plot(extra_train_file[0], c='#1f77b4')\n",
    "#ax[1,0].plot(extra_val_file[0], c='#1f77b4')\n",
    "#ax[0,0].set_title('Bag of N gram N = 1') \n",
    "#ax[0,i].plot(train_plot_file[i-1])\n",
    "#ax[1,i].plot(val_plot_file[i-1])\n",
    "#ax[0,i].set_xlabel(\"Training data size\")\n",
    "#ax[1,i].set_xlabel(\"Epochs\")\n",
    "#ax[0,i].set_ylabel(\"Training loss\")\n",
    "#ax[1,i].set_ylabel(\"Evaluation accuracy\")\n",
    "#ax[1,i].set_ylim([60,90])\n",
    "#ax[0,i].set_title(\"Bag of N grams, N = {}\".format(vocab_list[i-1]))\n",
    "ax[0,0].plot(train_plot_file[0])\n",
    "ax[1,0].plot(val_plot_file[0])\n",
    "ax[0,1].plot(extra_train_file)\n",
    "ax[1,1].plot(extra_val_file)\n",
    "ax[0,0].set_title('Tokenization removing punctuation and lower case')\n",
    "ax[0,1].set_title('Tokenization upper case')\n",
    "ax[0,0].set_xlabel('Training data size')\n",
    "ax[0,1].set_xlabel('Training data size')\n",
    "ax[1,0].set_xlabel('Epochs')\n",
    "ax[1,1].set_xlabel('Epochs')\n",
    "ax[0,0].set_ylabel('Training loss')\n",
    "ax[0,1].set_ylabel('Training loss')\n",
    "ax[1,0].set_ylabel('Evaluation accuracy')\n",
    "#ax[1,0].set_ylim = [50,90]\n",
    "#ax[1,1].set_ylim = [50,150]\n",
    "ax[1,1].set_ylabel('Evaluation accuracy')\n",
    "\n",
    "fig.savefig('./new_tokens_lower/tk_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file1 = pkl.load(open('all_eval_acc_2_vocab_100', 'rb'))\n",
    "file2 = pkl.load(open('all_eval_acc_2_vocab_200.p', 'rb'))\n",
    "file3 = pkl.load(open('all_eval_acc_2_vocab_300.p', 'rb'))\n",
    "file4 = pkl.load(open('all_eval_acc_2_vocab_400.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 8, figsize=(48,4))\n",
    "for i in range(0,8):\n",
    "    ax[i].plot(file1[i], label = 'emb_dim = 100')\n",
    "    ax[i].plot(file2[i], label = 'emb_dim = 200')\n",
    "    ax[i].plot(file3[i], label = 'emb_dim = 300')\n",
    "    ax[i].plot(file4[i], label = 'emb_dim = 400')\n",
    "    ax[i].set_title('The vocab size = {}'.format(10000*(i+1)))\n",
    "    ax[i].set_xlabel('Epochs')\n",
    "    ax[i].set_ylabel('Evaluation Accuracy')\n",
    "    ax[i].legend()\n",
    "fig.savefig('./new_tokens_lower/ultimate_graph.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
