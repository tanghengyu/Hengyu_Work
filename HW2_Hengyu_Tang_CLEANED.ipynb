{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the tables\n",
    "datadir = os.getcwd()\n",
    "snli_train_data = pd.read_table(datadir + '/snli_train.tsv')\n",
    "snli_val_data = pd.read_table(datadir+'/snli_val.tsv')\n",
    "test_data = pd.read_table(datadir + '/mnli_val.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load 50,000 pre-trained word embeddings\n",
    "#Also return the word2id and id2word and the entire vocab\n",
    "datadir = os.getcwd()\n",
    "words_to_load = 50000\n",
    "emb_size = 300\n",
    "with open(datadir + '/dataset_ml_2/hw2_data/wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings = np.zeros(((words_to_load+2), 300))\n",
    "    word2id = {}\n",
    "    id2words = {}\n",
    "    \n",
    "    id2words[PAD_IDX] = '<pad>'\n",
    "    id2words[UNK_IDX] = '<unk>'\n",
    "    word2id['<pad>'] = PAD_IDX\n",
    "    word2id['<unk>'] = UNK_IDX\n",
    "    \n",
    "    ordered_words= []\n",
    "    ordered_words.append('<pad>')\n",
    "    ordered_words.append('<unk>')\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load:\n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings[i+2,:] = np.asarray(s[1:])\n",
    "        word2id[s[0]] = i+2 #for extra pad and unk \n",
    "        id2words[i+2] = s[0]\n",
    "        ordered_words.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence2id(sentence_list):\n",
    "    id_list = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence_id_list = [word2id[word] if word in word2id else UNK_IDX for word in sentence]\n",
    "        id_list.append(sentence_id_list)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['genre'].unique()\n",
    "#Split the 5 genres\n",
    "test_data_fiction = test_data[test_data['genre'] == 'fiction']\n",
    "test_data_telephone = test_data[test_data['genre'] == 'telephone']\n",
    "test_data_slate = test_data[test_data['genre'] == 'slate']\n",
    "test_data_government = test_data[test_data['genre'] == 'government']\n",
    "test_data_travel = test_data[test_data['genre'] == 'travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Form two lists for train and val data, will deal with test data later when training is done\n",
    "sentence1_train_data = []\n",
    "sentence2_train_data = []\n",
    "sentence1_val_data = []\n",
    "sentence2_val_data = []\n",
    "\n",
    "for line in snli_train_data.sentence1:\n",
    "    sentence1_train_data.append(line.split())\n",
    "    \n",
    "for line in snli_train_data.sentence2:\n",
    "    sentence2_train_data.append(line.split())\n",
    "    \n",
    "for line in snli_val_data.sentence1:\n",
    "    sentence1_val_data.append(line.split())\n",
    "    \n",
    "for line in snli_val_data.sentence2:\n",
    "    sentence2_val_data.append(line.split())\n",
    "\n",
    "train_target = []\n",
    "val_target = []\n",
    "for label in snli_train_data.label:\n",
    "    if label == 'entailment':\n",
    "        train_target.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        train_target.append(1)\n",
    "    elif label == 'neutral':\n",
    "        train_target.append(2)\n",
    "\n",
    "        \n",
    "for label in snli_val_data.label:\n",
    "    if label == 'entailment':\n",
    "        val_target.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        val_target.append(1)\n",
    "    elif label == 'neutral':\n",
    "        val_target.append(2)\n",
    "\n",
    "#Write the processed tokens\n",
    "# pkl.dump(sentence1_train_data, open('./train_1_tokens.p', 'wb'))\n",
    "# pkl.dump(sentence2_train_data, open('./train_2_tokens.p', 'wb'))\n",
    "# pkl.dump(sentence1_val_data, open('./val_1_tokens.p', 'wb'))\n",
    "# pkl.dump(sentence2_val_data, open('./val_2_tokens.p', 'wb'))\n",
    "# pkl.dump(train_target, open('./train_target.p', 'wb'))\n",
    "# pkl.dump(val_target, open('./val_target.p', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the train and val data (only need to load this)\n",
    "sentence1_train_data = pkl.load(open(datadir+'/dataset_ml_2/train_1_tokens.p', 'rb'))\n",
    "sentence2_train_data = pkl.load(open(datadir+'/dataset_ml_2/train_2_tokens.p', 'rb'))\n",
    "sentence1_val_data = pkl.load(open(datadir+'/dataset_ml_2/val_1_tokens.p', 'rb'))\n",
    "sentence2_val_data = pkl.load(open(datadir+'/dataset_ml_2/val_2_tokens.p', 'rb'))\n",
    "train_target = pkl.load(open(datadir+'/dataset_ml_2/train_target.p', 'rb'))\n",
    "val_target = pkl.load(open(datadir+'/dataset_ml_2/val_target.p', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transfer into integer index vector by table look up\n",
    "sentence1_train_id = sentence2id(sentence1_train_data)\n",
    "max_length1 = max(len(sentence1_train_id[i]) for i in range(0,len(sentence1_train_id)))\n",
    "sentence2_train_id = sentence2id(sentence2_train_data)\n",
    "max_length2 = max(len(sentence2_train_id[i]) for i in range(0,len(sentence2_train_id)))\n",
    "\n",
    "sentence1_val_id = sentence2id(sentence1_val_data)\n",
    "max_length3 = max(len(sentence1_val_id[i]) for i in range(0,len(sentence1_val_id)))\n",
    "\n",
    "sentence2_val_id = sentence2id(sentence2_val_data)\n",
    "max_length4 = max(len(sentence2_val_id[i]) for i in range(0,len(sentence2_val_id)))\n",
    "\n",
    "MAX_SENTENCE_LENGTH = max(max_length1, max_length2)\n",
    "MAX_SENTENCE_LENGTH_1 = max(max_length3, max_length4)\n",
    "emb_size = 300\n",
    "\n",
    "\n",
    "MAX_SENTENCE_LENGTH, MAX_SENTENCE_LENGTH_1#should be (82,53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dataclass needed\n",
    "class SnliDataset(Dataset):\n",
    "    def __init__(self, sentence1_id, sentence2_id, target_list):\n",
    "        self.sentence1_id = sentence1_id\n",
    "        self.sentence2_id = sentence2_id\n",
    "        self.target_list = target_list\n",
    "        assert(len(self.sentence1_id) == len(self.target_list) & len(self.sentence2_id) == len(self.target_list))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        content1 = self.sentence1_id[index][:MAX_SENTENCE_LENGTH]\n",
    "        content2 = self.sentence2_id[index][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[index]\n",
    "        return [content1, len(content1), content2, len(content2), label]\n",
    "    \n",
    "def vocab_collate_func(batch):\n",
    "    sentence1_list = []\n",
    "    sentence2_list = []\n",
    "    \n",
    "    label_list = []\n",
    "    length1_list = []\n",
    "    length2_list = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length1_list.append(datum[1])\n",
    "        length2_list.append(datum[3])\n",
    "    \n",
    "    for datum in batch:\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]),\n",
    "                              pad_width = ((0, MAX_SENTENCE_LENGTH - datum[1])),\n",
    "                              mode = 'constant', constant_values = 0)\n",
    "        padded_vec_2 = np.pad(np.array(datum[2]),\n",
    "                              pad_width = ((0, MAX_SENTENCE_LENGTH - datum[3])),\n",
    "                              mode = 'constant', constant_values = 0)\n",
    "        sentence1_list.append(padded_vec_1)\n",
    "        sentence2_list.append(padded_vec_2)\n",
    "        \n",
    "\n",
    "    return [torch.from_numpy(np.array(sentence1_list)).cuda(),\n",
    "            torch.LongTensor(length1_list).cuda(), \n",
    "            torch.from_numpy(np.array(sentence2_list)).cuda(),\n",
    "            torch.LongTensor(length2_list).cuda(),\n",
    "            torch.LongTensor(label_list).cuda()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will do CNN first\n",
    "### 1. concatenation with different hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(loss_list):\n",
    "    fig, ax = plt.subplots(figsize = (12,10))\n",
    "    ax.plot(loss_list)\n",
    "    #fig.show()\n",
    "\n",
    "def test_model_cnn(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_1, length_1, data_2, length_2, labels in loader:\n",
    "        data_1_batch, length_1_batch, data_2_batch, length_2_batch,labels_batch = data_1, length_1, data_2, length_2, labels\n",
    "        data_1_batch.cuda()\n",
    "        data_2_batch.cuda()\n",
    "        length_1_batch.cuda()\n",
    "        length_2_batch.cuda()\n",
    "        labels_batch.cuda()\n",
    "\n",
    "        outputs = model(data_1_batch, data_2_batch)\n",
    "        \n",
    "        outputs = F.softmax(outputs, dim = 1)\n",
    "   \n",
    "        predicted = outputs.max(1, keepdim = True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "        \n",
    "        return (100 * correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, kernel_size, num_layers, num_classes, loaded_embedding):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = loaded_embedding\n",
    "        self.conv1_1 = nn.Conv1d(emb_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "        self.conv2_1 = nn.Conv1d(hidden_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "        \n",
    "        self.conv1_2 = nn.Conv1d(emb_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "        self.conv2_2 = nn.Conv1d(hidden_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size).double().cuda()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes).double().cuda()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "\n",
    "        embed_1 = torch.from_numpy(self.embedding[x1]).cuda()\n",
    "  \n",
    "        hidden_1 = self.conv1_1(embed_1.transpose(1,2)).transpose(1,2).cuda() \n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1,hidden_1.size(-1))).view(batch_size, hidden_1.size(-2), hidden_1.size(-1)).cuda()\n",
    "        \n",
    "        hidden_1 = self.conv2_1(hidden_1.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, hidden_1.size(-2), hidden_1.size(-1)).cuda()\n",
    "        \n",
    "        m_pool = nn.MaxPool1d(kernel_size = hidden_1.size()[1]).cuda()#Pooling over the time dimension\n",
    "        hidden_1 = m_pool(hidden_1.transpose(1,2)).transpose(1,2).cuda()#Change the dimension and change back\n",
    "        hidden_1 = hidden_1.squeeze(dim=1).cuda()#Drop the time dimension\n",
    "\n",
    "        \n",
    "        embed_2 = torch.from_numpy(self.embedding[x2]).cuda()\n",
    "        hidden_2 = self.conv1_2(embed_2.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, hidden_2.size(-2), hidden_2.size(-1)).cuda()\n",
    "        \n",
    "        hidden_2 = self.conv2_2(hidden_2.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, hidden_2.size(-2), hidden_2.size(-1)).cuda()\n",
    "        \n",
    "\n",
    "        hidden_2 = m_pool(hidden_2.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_2 = hidden_2.squeeze(dim=1).cuda()\n",
    "        \n",
    "        hidden = torch.cat((hidden_1, hidden_2),dim = 1).cuda()\n",
    "        #print('The concat hidden out size is {}'.format(hidden.size()))\n",
    "        \n",
    "        hidden = self.fc1(hidden).cuda()\n",
    "        hidden = F.relu(hidden, inplace = True)\n",
    "        logits = self.fc2(hidden).cuda()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Test for models\n",
    "model_cnn = CNN(emb_size = 300, hidden_size = 600, kernel_size = 3, num_layers = 2, num_classes = 3, \n",
    "           loaded_embedding = loaded_embeddings)\n",
    "model_cnn = model_cnn.double()\n",
    "model_cnn = model_cnn.cuda()\n",
    "\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 6;\n",
    "num_classes = 3;\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "train_loss_list = []\n",
    "for i in range(0,500):\n",
    "  #print(i)\n",
    "    model_cnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_cnn(data_1, data_2).cuda()\n",
    "    loss = criterion(outputs, labels).cuda()\n",
    "    train_loss_list.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "model_gru_mul.eval()\n",
    "outputs = model_gru_mul(data_1, data_2, length_1, length_2).cuda()\n",
    "        \n",
    "outputs = F.softmax(outputs, dim = 1)\n",
    "\n",
    "predicted = outputs.max(1, keepdim = True)[1]\n",
    "\n",
    "total += labels.size(0)\n",
    "correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "\n",
    "print('The train accuracy is {}'.format(100 * correct/total))\n",
    "plot_learning_curve(train_loss_list)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_list = [200,400,600, 800, 1000]\n",
    "#hidden_list = [200,400]\n",
    "\n",
    "\n",
    "total_train_loss_list = []\n",
    "total_val_acc_list = []\n",
    "total_final_val_acc = []\n",
    "\n",
    "for hidden_size in hidden_list:\n",
    "    print('The current hidden size is {}'.format(hidden_size))\n",
    "  \n",
    "  #Create new loader:\n",
    "    train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "    val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "  \n",
    "  #Create new models\n",
    "    model_cnn = CNN(emb_size = 300, hidden_size = hidden_size, kernel_size = 3, num_layers = 2, num_classes = 3, \n",
    "           loaded_embedding = loaded_embeddings)\n",
    "    model_cnn = model_cnn.double()\n",
    "    model_cnn = model_cnn.cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_cnn.parameters(), lr = learning_rate)\n",
    "    num_epochs = 10;\n",
    "  #num_classes = 3;\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "            model_cnn.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_cnn(data_1, data_2).cuda()\n",
    "            loss = criterion(outputs, labels).cuda()\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                val_acc = test_model_cnn(val_loader, model_cnn)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                      epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "  \n",
    "    print('end of epoch iterations')\n",
    "    total_train_loss_list.append(train_loss_list)\n",
    "    total_val_acc_list.append(val_acc_list)\n",
    "  \n",
    "    final_val_acc = test_model_cnn(val_loader, model_cnn)\n",
    "    total_final_val_acc.append(final_val_acc)\n",
    "\n",
    "print('The length of total_final_val_acc is {}, should be 5'.format(len(total_final_val_acc)))\n",
    "print('The length of total_train_loss is {}, should be 5, the lenfth of 1 should be 3125 x 10 {}'.format(len(total_train_loss_list), \n",
    "                                                                                               len(total_train_loss_list[0])))\n",
    "print('The length of total_val_loss is {}, should be 5, the lenfth of 1 should be 60 {}'.format(len(total_val_acc_list), \n",
    "                                                                                               len(total_val_acc_list[0])))\n",
    "\n",
    "  \n",
    "pkl.dump(total_train_loss_list, open('cnn_cat_train_loss.p', 'wb'))\n",
    "pkl.dump(total_val_acc_list, open('cnn_cat_val_acc.p', 'wb'))\n",
    "pkl.dump(total_final_val_acc, open('cnn_cat_final_val_acc.p', 'wb'))\n",
    "\n",
    "# files.download('cnn_cat_train_loss.p')\n",
    "\n",
    "# files.download('cnn_cat_val_acc.p')\n",
    "# files.download('cnn_cat_final_val_acc.p')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiplication with different hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, kernel_size, num_layers, num_classes, loaded_embedding):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = loaded_embedding\n",
    "        self.conv1_1 = nn.Conv1d(emb_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "        self.conv2_1 = nn.Conv1d(hidden_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "        \n",
    "        self.conv1_2 = nn.Conv1d(emb_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "        self.conv2_2 = nn.Conv1d(hidden_size, hidden_size, kernel_size =kernel_size, padding = 1).cuda()\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size).double().cuda()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes).double().cuda()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "\n",
    "        embed_1 = torch.from_numpy(self.embedding[x1]).cuda()\n",
    "  \n",
    "        hidden_1 = self.conv1_1(embed_1.transpose(1,2)).transpose(1,2).cuda() \n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1,hidden_1.size(-1))).view(batch_size, hidden_1.size(-2), hidden_1.size(-1)).cuda()\n",
    "        \n",
    "        hidden_1 = self.conv2_1(hidden_1.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, hidden_1.size(-2), hidden_1.size(-1)).cuda()\n",
    "        \n",
    "        m_pool = nn.MaxPool1d(kernel_size = hidden_1.size()[1]).cuda()#Pooling over the time dimension\n",
    "        hidden_1 = m_pool(hidden_1.transpose(1,2)).transpose(1,2).cuda()#Change the dimension and change back\n",
    "        hidden_1 = hidden_1.squeeze(dim=1).cuda()#Drop the time dimension\n",
    "\n",
    "        \n",
    "        embed_2 = torch.from_numpy(self.embedding[x2]).cuda()\n",
    "        hidden_2 = self.conv1_2(embed_2.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, hidden_2.size(-2), hidden_2.size(-1)).cuda()\n",
    "        \n",
    "        hidden_2 = self.conv2_2(hidden_2.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, hidden_2.size(-2), hidden_2.size(-1)).cuda()\n",
    "        \n",
    "\n",
    "        hidden_2 = m_pool(hidden_2.transpose(1,2)).transpose(1,2).cuda()\n",
    "        hidden_2 = hidden_2.squeeze(dim=1).cuda()\n",
    "        \n",
    "        #hidden = torch.cat((hidden_1, hidden_2),dim = 1).cuda()\n",
    "        hidden = torch.mul(hidden_1, hidden_2).cuda()\n",
    "        #print('The concat hidden out size is {}'.format(hidden.size()))\n",
    "        \n",
    "        hidden = self.fc1(hidden).cuda()\n",
    "        hidden = F.relu(hidden, inplace = True)\n",
    "        logits = self.fc2(hidden).cuda()\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_list = [200,400,600, 800, 1000]\n",
    "#hidden_list = [200,400]\n",
    "\n",
    "\n",
    "total_train_loss_list = []\n",
    "total_val_acc_list = []\n",
    "total_final_val_acc = []\n",
    "\n",
    "for hidden_size in hidden_list:\n",
    "    print('The current hidden size is {}'.format(hidden_size))\n",
    "  \n",
    "  #Create new loader:\n",
    "    train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "    val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "  \n",
    "  #Create new models\n",
    "    model_cnn = CNN(emb_size = 300, hidden_size = hidden_size, kernel_size = 3, num_layers = 2, num_classes = 3, \n",
    "           loaded_embedding = loaded_embeddings)\n",
    "    model_cnn = model_cnn.double()\n",
    "    model_cnn = model_cnn.cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_cnn.parameters(), lr = learning_rate)\n",
    "    num_epochs = 10;\n",
    "  #num_classes = 3;\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "            model_cnn.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_cnn(data_1, data_2).cuda()\n",
    "            loss = criterion(outputs, labels).cuda()\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                val_acc = test_model_cnn(val_loader, model_cnn)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                      epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "  \n",
    "    print('end of epoch iterations')\n",
    "    total_train_loss_list.append(train_loss_list)\n",
    "    total_val_acc_list.append(val_acc_list)\n",
    "  \n",
    "    final_val_acc = test_model_cnn(val_loader, model_cnn)\n",
    "    total_final_val_acc.append(final_val_acc)\n",
    "\n",
    "print('The length of total_final_val_acc is {}, should be 5'.format(len(total_final_val_acc)))\n",
    "print('The length of total_train_loss is {}, should be 5, the lenfth of 1 should be 3125 x 10 {}'.format(len(total_train_loss_list), \n",
    "                                                                                               len(total_train_loss_list[0])))\n",
    "print('The length of total_val_loss is {}, should be 5, the lenfth of 1 should be 60 {}'.format(len(total_val_acc_list), \n",
    "                                                                                               len(total_val_acc_list[0])))\n",
    "\n",
    "  \n",
    "pkl.dump(total_train_loss_list, open('cnn_mul_train_loss.p', 'wb'))\n",
    "pkl.dump(total_val_acc_list, open('cnn_mul_val_acc.p', 'wb'))\n",
    "pkl.dump(total_final_val_acc, open('cnn_mul_final_val_acc.p', 'wb'))\n",
    "\n",
    "# files.download('cnn_mul_train_loss.p')\n",
    "\n",
    "# files.download('cnn_mul_val_acc.p')\n",
    "# files.download('cnn_mul_final_val_acc.p')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have chosen based on validation accuracy to have multiplication with hidden size 600\n",
    "### Now we test on different kernel sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOw test for different kernel size\n",
    "\n",
    "kernel_list = [3, 5, 10,15, 20,25]\n",
    "\n",
    "total_train_loss_list = []\n",
    "total_val_acc_list = []\n",
    "total_final_val_acc = []\n",
    "\n",
    "for kernel_size in kernel_list:\n",
    "    print('The current kernel size is {}'.format(kernel_size))\n",
    "\n",
    "    #Create new loader:\n",
    "    train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "    val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "\n",
    "    #Create new models\n",
    "    model_cnn = CNN(emb_size = 300, hidden_size = 600, kernel_size = kernel_size, num_layers = 2, num_classes = 3, \n",
    "           loaded_embedding = loaded_embeddings)\n",
    "    model_cnn = model_cnn.double()\n",
    "    model_cnn = model_cnn.cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_cnn.parameters(), lr = learning_rate)\n",
    "    num_epochs = 10;\n",
    "    #num_classes = 3;\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "            model_cnn.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_cnn(data_1, data_2).cuda()\n",
    "            loss = criterion(outputs, labels).cuda()\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                val_acc = test_model_cnn(val_loader, model_cnn)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                          epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "  \n",
    "    print('end of epoch iterations')\n",
    "    total_train_loss_list.append(train_loss_list)\n",
    "    total_val_acc_list.append(val_acc_list)\n",
    "  \n",
    "    final_val_acc = test_model_cnn(val_loader, model_cnn)\n",
    "    total_final_val_acc.append(final_val_acc)\n",
    "\n",
    "print('The length of total_final_val_acc is {}, should be 5'.format(len(total_final_val_acc)))\n",
    "print('The length of total_train_loss is {}, should be 5, the lenfth of 1 should be 3125 x 10 {}'.format(len(total_train_loss_list), \n",
    "                                                                                               len(total_train_loss_list[0])))\n",
    "print('The length of total_val_loss is {}, should be 5, the lenfth of 1 should be 60 {}'.format(len(total_val_acc_list), \n",
    "                                                                                               len(total_val_acc_list[0])))\n",
    "\n",
    "  \n",
    "pkl.dump(total_train_loss_list, open('cnn_kernel_train_loss.p', 'wb'))\n",
    "pkl.dump(total_val_acc_list, open('cnn_kernel_val_acc.p', 'wb'))\n",
    "pkl.dump(total_final_val_acc, open('cnn_kernel_final_val_acc.p', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We conclude based on validation accuracy that the optimum model is CNN with multiplication, hidden size = 600, kerneksize = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean test data and remove outliers\n",
    "test_data_fiction_cln = test_data_fiction.drop([4377,2920],0)#Remove the 2 abnormal one\n",
    "\n",
    "#Form two lists - will be repeated later for different genre\n",
    "sentence1_test_data = []\n",
    "sentence2_test_data = []\n",
    "    \n",
    "for line in test_data_fiction_cln.sentence1:\n",
    "    sentence1_test_data.append(line.split())\n",
    "    \n",
    "for line in test_data_fiction_cln.sentence2:\n",
    "    sentence2_test_data.append(line.split())\n",
    "\n",
    "test_target = []\n",
    "for label in test_data_fiction_cln.label:\n",
    "    if label == 'entailment':\n",
    "        test_target.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        test_target.append(1)\n",
    "    elif label == 'neutral':\n",
    "        test_target.append(2)\n",
    "\n",
    "sentence1_test_id = sentence2id(sentence1_test_data)\n",
    "max_length1 = max(len(sentence1_test_id[i]) for i in range(0,len(sentence1_test_id)))\n",
    "sentence2_test_id = sentence2id(sentence2_test_data)\n",
    "max_length2 = max(len(sentence2_test_id[i]) for i in range(0,len(sentence2_test_id)))\n",
    "\n",
    "\n",
    "MAX_SENTENCE_LENGTH = max(max_length1, max_length2)\n",
    "emb_size = 300\n",
    "\n",
    "\n",
    "MAX_SENTENCE_LENGTH #Should be 55\n",
    "\n",
    "\n",
    "test_dataset = SnliDataset(sentence1_test_id, sentence2_test_id, test_target)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         collate_fn = vocab_collate_func,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_data):\n",
    "    sentence1_test_data = []\n",
    "    sentence2_test_data = []\n",
    "\n",
    "    for line in test_data.sentence1:\n",
    "        sentence1_test_data.append(line.split())\n",
    "\n",
    "    for line in test_data.sentence2:\n",
    "        sentence2_test_data.append(line.split())\n",
    "\n",
    "    test_target = []\n",
    "    for label in test_data.label:\n",
    "        if label == 'entailment':\n",
    "            test_target.append(0)\n",
    "        elif label == 'contradiction':\n",
    "            test_target.append(1)\n",
    "        elif label == 'neutral':\n",
    "            test_target.append(2)\n",
    "\n",
    "    sentence1_test_id = sentence2id(sentence1_test_data)\n",
    "    max_length1 = max(len(sentence1_test_id[i]) for i in range(0,len(sentence1_test_id)))\n",
    "    sentence2_test_id = sentence2id(sentence2_test_data)\n",
    "    max_length2 = max(len(sentence2_test_id[i]) for i in range(0,len(sentence2_test_id)))\n",
    "\n",
    "    MAX_SENTENCE_LENGTH = max(max_length1, max_length2)\n",
    "    \n",
    "    test_dataset = SnliDataset(sentence1_test_id, sentence2_test_id, test_target)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         collate_fn = vocab_collate_func,\n",
    "                                         shuffle = False)\n",
    "    return MAX_SENTENCE_LENGTH, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_telephone)\n",
    "test_acc_telephone_cnn = test_model_cnn(test_loader, model_cnn)\n",
    "#test_acc_telephone_cnn\n",
    "#print(len(test_loader))\n",
    "#print(should be 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_idx = 0\n",
    "# sec_len = 0\n",
    "# max_len = 0\n",
    "# max_sen = ''\n",
    "# for i in range(len(sentence1_test_data)):\n",
    "#   curr_len = len(sentence1_test_data[i])\n",
    "#   if curr_len > max_len:\n",
    "#     sec_len = max_len\n",
    "#     max_idx = i\n",
    "#     max_len = curr_len\n",
    "#     max_sen = sentence1_test_data[i]\n",
    "    \n",
    "# #THe sec len iso only 53, there we consider remove the abnormal sentence as it will result in excessive paddings\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_slate)\n",
    "test_acc_slate_cnn = test_model_cnn(test_loader, model_cnn)\n",
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_government)\n",
    "test_acc_government_cnn = test_model_cnn(test_loader, model_cnn)\n",
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_travel)\n",
    "test_acc_travel_cnn = test_model_cnn(test_loader, model_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_cnn_extract(loader, model):\n",
    "    \n",
    "    model.eval()\n",
    "    for i, (data_1, length_1, data_2, length_2, labels) in enumerate(loader):\n",
    "        #TAKE THE second batch different from RNN\n",
    "        if i == 2:\n",
    "            break\n",
    "      \n",
    "    data_1_batch, length_1_batch, data_2_batch, length_2_batch,labels_batch = data_1, length_1, data_2, length_2, labels\n",
    "    data_1_batch.cuda()\n",
    "    data_2_batch.cuda()\n",
    "    length_1_batch.cuda()\n",
    "    length_2_batch.cuda()\n",
    "    labels_batch.cuda()\n",
    "\n",
    "    outputs = model(data_1_batch, data_2_batch)\n",
    "\n",
    "    outputs = F.softmax(outputs, dim = 1)\n",
    "\n",
    "    predicted = outputs.max(1, keepdim = True)[1]\n",
    "\n",
    "    result = predicted.eq(labels.view_as(predicted))\n",
    "        \n",
    "    return data_1, data_2, labels, predicted, result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now RNN\n",
    "### 1. concatenation with different hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test function defined here for all gru modesl\n",
    "def plot_learning_curve(loss_list):\n",
    "    fig, ax = plt.subplots(figsize = (12,10))\n",
    "    ax.plot(loss_list)\n",
    "  #fig.show()\n",
    "  \n",
    "def test_model_gru(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for data_1, length_1, data_2, length_2, labels in loader:\n",
    "        data_1_batch, length_1_batch, data_2_batch, length_2_batch,labels_batch = data_1, length_1, data_2, length_2, labels\n",
    "        data_1_batch = data_1_batch.cuda()\n",
    "        data_2_batch = data_2_batch.cuda()\n",
    "        length_1_batch = length_1_batch.cuda()\n",
    "        length_2_batch = length_2_batch.cuda()\n",
    "        labels_batch = labels_batch.cuda()\n",
    "\n",
    "    outputs = model(data_1_batch, data_2_batch, length_1_batch, length_2_batch).cuda()\n",
    "\n",
    "    outputs = F.softmax(outputs, dim = 1)\n",
    "\n",
    "    predicted = outputs.max(1, keepdim = True)[1].cuda()\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "\n",
    "    return (100 * correct/total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRU bidirectional class with concatenations\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, loaded_embedding):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = loaded_embedding\n",
    "        self.GRU = nn.GRU(emb_size, hidden_size, num_layers, batch_first = True, bidirectional = True).cuda()\n",
    "        \n",
    "        #Bidirectional and concatenate two together\n",
    "        self.fc1 = nn.Linear(4*hidden_size, hidden_size).float().cuda()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes).float().cuda()\n",
    "       \n",
    "\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden = torch.randn((self.num_layers*2), batch_size, self.hidden_size).cuda()\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x1, x2, length1, length2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "   \n",
    "        self.hidden_1 = self.init_hidden(batch_size).cuda()\n",
    "    \n",
    "        length1_sorted, sorted_idx_1 = torch.sort(length1, descending = True)\n",
    "        length1_sorted.cuda()\n",
    "        sorted_idx_1.cuda()\n",
    "        \n",
    "        _, unsorted_idx_1 = torch.sort(sorted_idx_1)\n",
    "        unsorted_idx_1.cuda()\n",
    "        x1_sorted = x1.index_select(dim=0, index = sorted_idx_1).cuda()\n",
    "        \n",
    "        embed_1 = torch.from_numpy(self.embedding[x1_sorted]).cuda()\n",
    "        \n",
    "        length1_sorted_copy = length1_sorted.cpu()\n",
    "        pac_seq_1 = torch.nn.utils.rnn.pack_padded_sequence(embed_1, length1_sorted_copy.numpy(), batch_first = True).float()\n",
    "\n",
    "        _, self.hidden_1 = self.GRU(pac_seq_1, self.hidden_1)\n",
    "        self.hidden_1.cuda()\n",
    "        \n",
    "        first_dir = self.hidden_1.narrow(0,0,1).squeeze(dim=0).cuda()\n",
    "        sec_dir = self.hidden_1.narrow(0,1,1).squeeze(dim=0).cuda()\n",
    "\n",
    "        output_1 = torch.cat((first_dir, sec_dir), dim=1).cuda()\n",
    "\n",
    "        output_1 = output_1.index_select(dim=0,index=unsorted_idx_1).cuda()\n",
    "\n",
    "                \n",
    "        self.hidden_2 = self.init_hidden(batch_size).cuda()\n",
    "\n",
    "        length2_sorted, sorted_idx_2 = torch.sort(length2, descending = True)\n",
    "        length2_sorted.cuda()\n",
    "        sorted_idx_2.cuda()\n",
    "        \n",
    "        _, unsorted_idx_2 = torch.sort(sorted_idx_2)\n",
    "        unsorted_idx_2.cuda()\n",
    "        x2_sorted = x2.index_select(dim=0, index = sorted_idx_2).cuda()\n",
    "        \n",
    "        embed_2 = torch.from_numpy(self.embedding[x2_sorted]).cuda()\n",
    "        \n",
    "        length2_sorted_copy = length2_sorted.cpu()\n",
    "        pac_seq_2 = torch.nn.utils.rnn.pack_padded_sequence(embed_2, length2_sorted_copy.numpy(), batch_first = True).float()\n",
    "        \n",
    "        _, self.hidden_2 = self.GRU(pac_seq_2, self.hidden_2)\n",
    "        self.hidden_2.cuda()\n",
    "        first_dir_2 = self.hidden_2.narrow(0,0,1).squeeze(dim=0).cuda()\n",
    "        sec_dir_2 = self.hidden_2.narrow(0,1,1).squeeze(dim=0).cuda()\n",
    "       \n",
    "        output_2 = torch.cat((first_dir_2, sec_dir_2), dim=1).cuda()\n",
    "\n",
    "        output_2 = output_2.index_select(dim=0,index=unsorted_idx_2).cuda()\n",
    "        \n",
    "        rnn_out = torch.cat((output_1, output_2), dim = 1).cuda()# 32 x (400x2)\n",
    "\n",
    "        rnn_out = self.fc1(rnn_out).cuda()\n",
    "        rnn_out = F.relu(rnn_out, inplace = True)\n",
    "        logits = self.fc2(rnn_out).cuda()\n",
    "        \n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Brief test\n",
    "train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "\n",
    "model_gru_mul = GRU(emb_size = 300, hidden_size = 600, num_layers = 1, num_classes = 3, loaded_embedding = loaded_embeddings)\n",
    "model_gru_mul = model_gru_mul.cuda()\n",
    "\n",
    "#Test code for small minibatches\n",
    "##Test for models\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 6;\n",
    "num_classes = 3;xq\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.Adam(model_gru_mul.parameters(), lr = learning_rate)\n",
    "\n",
    "train_loss_list = []\n",
    "#Get a small batch\n",
    "for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "    if i == 0: \n",
    "        break\n",
    "\n",
    "train_loss_list = []\n",
    "for i in range(0,500):\n",
    "  #print(i)\n",
    "    model_gru_mul.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model_gru_mul(data_1, data_2, length_1, length_2).cuda()\n",
    "    loss = criterion(outputs, labels).cuda()\n",
    "    train_loss_list.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "model_gru_mul.eval()\n",
    "outputs = model_gru_mul(data_1, data_2, length_1, length_2).cuda()\n",
    "        \n",
    "outputs = F.softmax(outputs, dim = 1)\n",
    "\n",
    "predicted = outputs.max(1, keepdim = True)[1]\n",
    "\n",
    "total += labels.size(0)\n",
    "correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "\n",
    "print('The train accuracy is {}'.format(100 * correct/total))\n",
    "plot_learning_curve(train_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train for different hidden size\n",
    "hidden_list = [200,400,600,800,1000]\n",
    "\n",
    "total_train_loss_list = []\n",
    "total_val_acc_list = []\n",
    "total_final_val_acc = []\n",
    "\n",
    "for hidden_size in hidden_list:\n",
    "    print('The current hidden size is {}'.format(hidden_size))\n",
    "  \n",
    "  #Create new loader:\n",
    "    train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "    val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "  \n",
    "  #Create new models\n",
    "    model_gru = GRU(emb_size = 300, hidden_size = hidden_size, num_layers = 1, num_classes = 3, loaded_embedding = loaded_embeddings).cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_gru.parameters(), lr = learning_rate)\n",
    "    num_epochs = 10;\n",
    "  #num_classes = 3;\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "            model_gru.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_gru(data_1, data_2, length_1, length_2).cuda()\n",
    "            loss = criterion(outputs, labels).cuda()\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                val_acc = test_model_gru(val_loader, model_gru)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                      epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "  \n",
    "    print('end of epoch iterations')\n",
    "    total_train_loss_list.append(train_loss_list)\n",
    "    total_val_acc_list.append(val_acc_list)\n",
    "  \n",
    "    final_val_acc = test_model_gru(val_loader, model_gru)\n",
    "    total_final_val_acc.append(final_val_acc)\n",
    "\n",
    "#print('The length of total_final_val_acc is {}, should be 5'.format(len(total_final_val_acc)))\n",
    "#print('The length of total_train_loss is {}, should be 5, the lenfth of 1 should be 3125 x 10 {}'.format(len(total_train_loss_list), \n",
    "#                                                                                                len(total_train_loss_list[0])))\n",
    "# print('The length of total_val_loss is {}, should be 5, the lenfth of 1 should be 60 {}'.format(len(total_val_acc_list), \n",
    "#                                                                                                len(total_val_acc_list[0])))\n",
    "\n",
    "pkl.dump(total_train_loss_list, open('hidden_size_train_loss.p', 'wb'))\n",
    "pkl.dump(total_val_acc_list, open('hidden_size_val_acc.p', 'wb'))\n",
    "pkl.dump(total_final_val_acc, open('total_final_val_acc.p', 'wb'))\n",
    "\n",
    "# files.download('hidden_size_train_loss.p')\n",
    "# files.download('hidden_size_val_acc.p')\n",
    "# files.download('total_final_val_acc.p')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now lets test on GRU with multiplication on different hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, loaded_embedding):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = loaded_embedding\n",
    "        self.GRU = nn.GRU(emb_size, hidden_size, num_layers, batch_first = True, bidirectional = True).cuda()\n",
    "        \n",
    "        #Bidirectional and concatenate two together\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size).float().cuda()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes).float().cuda()\n",
    "       \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden = torch.randn((self.num_layers*2), batch_size, self.hidden_size).cuda()\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x1, x2, length1, length2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "   \n",
    "        self.hidden_1 = self.init_hidden(batch_size).cuda()\n",
    "    \n",
    "        length1_sorted, sorted_idx_1 = torch.sort(length1, descending = True)\n",
    "        length1_sorted.cuda()\n",
    "        sorted_idx_1.cuda()\n",
    "        \n",
    "        _, unsorted_idx_1 = torch.sort(sorted_idx_1)\n",
    "        unsorted_idx_1.cuda()\n",
    "        x1_sorted = x1.index_select(dim=0, index = sorted_idx_1).cuda()\n",
    "        \n",
    "        embed_1 = torch.from_numpy(self.embedding[x1_sorted]).cuda()\n",
    "        \n",
    "        length1_sorted_copy = length1_sorted.cpu()\n",
    "        pac_seq_1 = torch.nn.utils.rnn.pack_padded_sequence(embed_1, length1_sorted_copy.numpy(), batch_first = True).float()\n",
    "\n",
    "        _, self.hidden_1 = self.GRU(pac_seq_1, self.hidden_1)\n",
    "        self.hidden_1.cuda()\n",
    "        \n",
    "        first_dir = self.hidden_1.narrow(0,0,1).squeeze(dim=0).cuda()\n",
    "        sec_dir = self.hidden_1.narrow(0,1,1).squeeze(dim=0).cuda()\n",
    "\n",
    "        output_1 = torch.cat((first_dir, sec_dir), dim=1).cuda()   \n",
    "        output_1 = output_1.index_select(dim=0,index=unsorted_idx_1).cuda()\n",
    "         \n",
    "          \n",
    "          \n",
    "        self.hidden_2 = self.init_hidden(batch_size).cuda()\n",
    "\n",
    "        length2_sorted, sorted_idx_2 = torch.sort(length2, descending = True)\n",
    "        length2_sorted.cuda()\n",
    "        sorted_idx_2.cuda()\n",
    "        \n",
    "        _, unsorted_idx_2 = torch.sort(sorted_idx_2)\n",
    "        unsorted_idx_2.cuda()\n",
    "        x2_sorted = x2.index_select(dim=0, index = sorted_idx_2).cuda()\n",
    "        \n",
    "        embed_2 = torch.from_numpy(self.embedding[x2_sorted]).cuda()\n",
    "        \n",
    "        length2_sorted_copy = length2_sorted.cpu()\n",
    "        pac_seq_2 = torch.nn.utils.rnn.pack_padded_sequence(embed_2, length2_sorted_copy.numpy(), batch_first = True).float()\n",
    "        \n",
    "        _, self.hidden_2 = self.GRU(pac_seq_2, self.hidden_2)\n",
    "        self.hidden_2.cuda()\n",
    "        first_dir_2 = self.hidden_2.narrow(0,0,1).squeeze(dim=0).cuda()\n",
    "        sec_dir_2 = self.hidden_2.narrow(0,1,1).squeeze(dim=0).cuda()\n",
    "       \n",
    "        output_2 = torch.cat((first_dir_2, sec_dir_2), dim=1).cuda()\n",
    "\n",
    "        output_2 = output_2.index_select(dim=0,index=unsorted_idx_2).cuda()\n",
    "        \n",
    "        #print('The shape of output_1 is {}'.format(output_1.size()))\n",
    "        #print('The shape of output_2 is {}'.format(output_2.size()))\n",
    "\n",
    "        rnn_out = torch.mul(output_1, 1, output_2).cuda()# 32 x 400\n",
    "        #print('The shape of rnn_output is {}'.format(rnn_out.size()))\n",
    "        \n",
    "        rnn_out = self.fc1(rnn_out).cuda()\n",
    "        rnn_out = F.relu(rnn_out, inplace = True)\n",
    "        logits = self.fc2(rnn_out).cuda()\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train for different hidden size\n",
    "hidden_list = [200,400,600,800,1000]\n",
    "\n",
    "total_train_loss_list = []\n",
    "total_val_acc_list = []\n",
    "total_final_val_acc = []\n",
    "\n",
    "for hidden_size in hidden_list:\n",
    "    print('The current hidden size is {}'.format(hidden_size))\n",
    "  \n",
    "  #Create new loader:\n",
    "    train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "    val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "  \n",
    "  #Create new models\n",
    "    model_gru = GRU(emb_size = 300, hidden_size = hidden_size, num_layers = 1, num_classes = 3, loaded_embedding = loaded_embeddings).cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_gru.parameters(), lr = learning_rate)\n",
    "    num_epochs = 10;\n",
    "  #num_classes = 3;\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "            model_gru.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_gru(data_1, data_2, length_1, length_2).cuda()\n",
    "            loss = criterion(outputs, labels).cuda()\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                val_acc = test_model_gru(val_loader, model_gru)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                      epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "  \n",
    "    print('end of epoch iterations')\n",
    "    total_train_loss_list.append(train_loss_list)\n",
    "    total_val_acc_list.append(val_acc_list)\n",
    "  \n",
    "    final_val_acc = test_model_gru(val_loader, model_gru)\n",
    "    total_final_val_acc.append(final_val_acc)                                                                                             len(total_val_acc_list[0])))\n",
    "\n",
    "pkl.dump(total_train_loss_list, open('mul_size_train_loss_rnn.p', 'wb'))\n",
    "pkl.dump(total_val_acc_list, open('mul_size_val_acc_rnn.p', 'wb'))\n",
    "pkl.dump(total_final_val_acc, open('mul_total_final_val_acc_rnn.p', 'wb'))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Now lets test on GRU with addition on different hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, loaded_embedding):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = loaded_embedding\n",
    "        self.GRU = nn.GRU(emb_size, hidden_size, num_layers, batch_first = True, bidirectional = True).cuda()\n",
    "        \n",
    "        #Bidirectional and concatenate two together\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size).float().cuda()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes).float().cuda()\n",
    "       \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden = torch.randn((self.num_layers*2), batch_size, self.hidden_size).cuda()\n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, x1, x2, length1, length2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "   \n",
    "        self.hidden_1 = self.init_hidden(batch_size).cuda()\n",
    "    \n",
    "        length1_sorted, sorted_idx_1 = torch.sort(length1, descending = True)\n",
    "        length1_sorted.cuda()\n",
    "        sorted_idx_1.cuda()\n",
    "        \n",
    "        _, unsorted_idx_1 = torch.sort(sorted_idx_1)\n",
    "        unsorted_idx_1.cuda()\n",
    "        x1_sorted = x1.index_select(dim=0, index = sorted_idx_1).cuda()\n",
    "        \n",
    "        embed_1 = torch.from_numpy(self.embedding[x1_sorted]).cuda()\n",
    "        \n",
    "        length1_sorted_copy = length1_sorted.cpu()\n",
    "        pac_seq_1 = torch.nn.utils.rnn.pack_padded_sequence(embed_1, length1_sorted_copy.numpy(), batch_first = True).float()\n",
    "\n",
    "        _, self.hidden_1 = self.GRU(pac_seq_1, self.hidden_1)\n",
    "        self.hidden_1.cuda()\n",
    "        \n",
    "        first_dir = self.hidden_1.narrow(0,0,1).squeeze(dim=0).cuda()\n",
    "        sec_dir = self.hidden_1.narrow(0,1,1).squeeze(dim=0).cuda()\n",
    "\n",
    "        output_1 = torch.cat((first_dir, sec_dir), dim=1).cuda()   \n",
    "        output_1 = output_1.index_select(dim=0,index=unsorted_idx_1).cuda()\n",
    "         \n",
    "          \n",
    "          \n",
    "        self.hidden_2 = self.init_hidden(batch_size).cuda()\n",
    "\n",
    "        length2_sorted, sorted_idx_2 = torch.sort(length2, descending = True)\n",
    "        length2_sorted.cuda()\n",
    "        sorted_idx_2.cuda()\n",
    "        \n",
    "        _, unsorted_idx_2 = torch.sort(sorted_idx_2)\n",
    "        unsorted_idx_2.cuda()\n",
    "        x2_sorted = x2.index_select(dim=0, index = sorted_idx_2).cuda()\n",
    "        \n",
    "        embed_2 = torch.from_numpy(self.embedding[x2_sorted]).cuda()\n",
    "        \n",
    "        length2_sorted_copy = length2_sorted.cpu()\n",
    "        pac_seq_2 = torch.nn.utils.rnn.pack_padded_sequence(embed_2, length2_sorted_copy.numpy(), batch_first = True).float()\n",
    "        \n",
    "        _, self.hidden_2 = self.GRU(pac_seq_2, self.hidden_2)\n",
    "        self.hidden_2.cuda()\n",
    "        first_dir_2 = self.hidden_2.narrow(0,0,1).squeeze(dim=0).cuda()\n",
    "        sec_dir_2 = self.hidden_2.narrow(0,1,1).squeeze(dim=0).cuda()\n",
    "       \n",
    "        output_2 = torch.cat((first_dir_2, sec_dir_2), dim=1).cuda()\n",
    "\n",
    "        output_2 = output_2.index_select(dim=0,index=unsorted_idx_2).cuda()\n",
    "        \n",
    "        #print('The shape of output_1 is {}'.format(output_1.size()))\n",
    "        #print('The shape of output_2 is {}'.format(output_2.size()))\n",
    "\n",
    "        rnn_out = torch.add(output_1, 1, output_2).cuda()# 32 x 400\n",
    "        #print('The shape of rnn_output is {}'.format(rnn_out.size()))\n",
    "        \n",
    "        rnn_out = self.fc1(rnn_out).cuda()\n",
    "        rnn_out = F.relu(rnn_out, inplace = True)\n",
    "        logits = self.fc2(rnn_out).cuda()\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train for different hidden size\n",
    "hidden_list = [200,400,600,800,1000]\n",
    "\n",
    "total_train_loss_list = []\n",
    "total_val_acc_list = []\n",
    "total_final_val_acc = []\n",
    "\n",
    "for hidden_size in hidden_list:\n",
    "    print('The current hidden size is {}'.format(hidden_size))\n",
    "  \n",
    "  #Create new loader:\n",
    "    train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          collate_fn = vocab_collate_func,\n",
    "                                          shuffle = True)\n",
    "    val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "  \n",
    "    #Create new models\n",
    "    model_gru = GRU(emb_size = 300, hidden_size = hidden_size, num_layers = 1, num_classes = 3, loaded_embedding = loaded_embeddings).cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_gru.parameters(), lr = learning_rate)\n",
    "    num_epochs = 10;\n",
    "    #num_classes = 3;\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_acc_list = []\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "            model_gru.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model_gru(data_1, data_2, length_1, length_2).cuda()\n",
    "            loss = criterion(outputs, labels).cuda()\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i > 0 and i % 500 == 0:\n",
    "                val_acc = test_model_gru(val_loader, model_gru)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                      epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "  \n",
    "    print('end of epoch iterations')\n",
    "    total_train_loss_list.append(train_loss_list)\n",
    "    total_val_acc_list.append(val_acc_list)\n",
    "  \n",
    "    final_val_acc = test_model_gru(val_loader, model_gru)\n",
    "    total_final_val_acc.append(final_val_acc)                                                                                             len(total_val_acc_list[0])))\n",
    "\n",
    "pkl.dump(total_train_loss_list, open('add_size_train_loss_rnn.p', 'wb'))\n",
    "pkl.dump(total_val_acc_list, open('add_size_val_acc_rnn.p', 'wb'))\n",
    "pkl.dump(total_final_val_acc, open('add_total_final_val_acc_rnn.p', 'wb'))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_cat = pkl.load(open('total_final_val_acc.p', 'rb'))\n",
    "val_mul = pkl.load(open('mul_total_final_val_acc_rnn.p', 'rb'))\n",
    "val_add = pkl.load(open('add_total_final_val_acc_rnn.p', 'rb'))\n",
    "val_cat, val_mul, val_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a result, I decided to adopt the concatenation at hidden size 60 to do the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean test data and remove outliers\n",
    "test_data_fiction_cln = test_data_fiction.drop([4377,2920],0)#Remove the 2 abnormal one\n",
    "\n",
    "#Form two lists - will be repeated later for different genre\n",
    "sentence1_test_data = []\n",
    "sentence2_test_data = []\n",
    "    \n",
    "for line in test_data_fiction_cln.sentence1:\n",
    "    sentence1_test_data.append(line.split())\n",
    "    \n",
    "for line in test_data_fiction_cln.sentence2:\n",
    "    sentence2_test_data.append(line.split())\n",
    "\n",
    "test_target = []\n",
    "for label in test_data_fiction_cln.label:\n",
    "    if label == 'entailment':\n",
    "        test_target.append(0)\n",
    "    elif label == 'contradiction':\n",
    "        test_target.append(1)\n",
    "    elif label == 'neutral':\n",
    "        test_target.append(2)\n",
    "\n",
    "sentence1_test_id = sentence2id(sentence1_test_data)\n",
    "max_length1 = max(len(sentence1_test_id[i]) for i in range(0,len(sentence1_test_id)))\n",
    "sentence2_test_id = sentence2id(sentence2_test_data)\n",
    "max_length2 = max(len(sentence2_test_id[i]) for i in range(0,len(sentence2_test_id)))\n",
    "\n",
    "\n",
    "MAX_SENTENCE_LENGTH = max(max_length1, max_length2)\n",
    "emb_size = 300\n",
    "\n",
    "\n",
    "MAX_SENTENCE_LENGTH #Should be 55\n",
    "\n",
    "\n",
    "test_dataset = SnliDataset(sentence1_test_id, sentence2_test_id, test_target)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         collate_fn = vocab_collate_func,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_idx = 0\n",
    "# sec_len = 0\n",
    "# max_len = 0\n",
    "# max_sen = ''\n",
    "# for i in range(len(sentence1_test_data)):\n",
    "#   curr_len = len(sentence1_test_data[i])\n",
    "#   if curr_len > max_len:\n",
    "#     sec_len = max_len\n",
    "#     max_idx = i\n",
    "#     max_len = curr_len\n",
    "#     max_sen = sentence1_test_data[i]\n",
    "    \n",
    "# #THe sec len iso only 53, there we consider remove the abnormal sentence as it will result in excessive paddings\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_data):\n",
    "    sentence1_test_data = []\n",
    "    sentence2_test_data = []\n",
    "\n",
    "    for line in test_data.sentence1:\n",
    "        sentence1_test_data.append(line.split())\n",
    "\n",
    "    for line in test_data.sentence2:\n",
    "        sentence2_test_data.append(line.split())\n",
    "\n",
    "    test_target = []\n",
    "    for label in test_data.label:\n",
    "        if label == 'entailment':\n",
    "            test_target.append(0)\n",
    "        elif label == 'contradiction':\n",
    "            test_target.append(1)\n",
    "        elif label == 'neutral':\n",
    "            test_target.append(2)\n",
    "\n",
    "    sentence1_test_id = sentence2id(sentence1_test_data)\n",
    "    max_length1 = max(len(sentence1_test_id[i]) for i in range(0,len(sentence1_test_id)))\n",
    "    sentence2_test_id = sentence2id(sentence2_test_data)\n",
    "    max_length2 = max(len(sentence2_test_id[i]) for i in range(0,len(sentence2_test_id)))\n",
    "\n",
    "    MAX_SENTENCE_LENGTH = max(max_length1, max_length2)\n",
    "    \n",
    "    test_dataset = SnliDataset(sentence1_test_id, sentence2_test_id, test_target)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         collate_fn = vocab_collate_func,\n",
    "                                         shuffle = False)\n",
    "    return MAX_SENTENCE_LENGTH, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = SnliDataset(sentence1_train_id, sentence2_train_id, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                        batch_size = BATCH_SIZE,\n",
    "                                        collate_fn = vocab_collate_func,\n",
    "                                        shuffle = True)\n",
    "val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                      batch_size = BATCH_SIZE,\n",
    "                                      collate_fn = vocab_collate_func,\n",
    "                                      shuffle = True)\n",
    "#Train the final models\n",
    "model_gru_mul = GRU(emb_size = 300, hidden_size = 400, num_layers = 1, \n",
    "                    num_classes = 3, loaded_embedding = loaded_embeddings).cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "learning_rate = 3e-4\n",
    "optimizer = torch.optim.Adam(model_gru_mul.parameters(), lr = learning_rate)\n",
    "num_epochs = 8;\n",
    "\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_1, length_1, data_2, length_2, labels) in enumerate(train_loader):\n",
    "        model_gru_mul.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_gru_mul(data_1, data_2, length_1, length_2).cuda()\n",
    "        loss = criterion(outputs, labels).cuda()\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i > 0 and i % 500 == 0:\n",
    "            val_acc = test_model_gru(val_loader, model_gru_mul)\n",
    "            val_acc_list.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                    epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "\n",
    "final_val_acc = test_model_gru(val_loader, model_gru_mul)\n",
    "final_test_acc = test_model_gru(test_loader, model_gru_mul)\n",
    "\n",
    "print('The test accuracy for fiction is {}, {}'.format(final_val_acc, final_test_acc))\n",
    "  \n",
    "# pkl.dump(train_loss_list, open('fiction_train.p', 'wb'))\n",
    "# pkl.dump(val_acc_list, open('fiction_val.p', 'wb'))\n",
    "# pkl.dump([final_val_acc, final_test_acc], open('fiction_test.p', 'wb'))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Little test to see abonormal inputs\n",
    "# max_idx = 0\n",
    "# sec_len = 0\n",
    "# max_len = 0\n",
    "# max_sen = ''\n",
    "# for i in range(len(sentence1_test_data)):\n",
    "#   curr_len = len(sentence1_test_data[i])\n",
    "#   if curr_len > max_len:\n",
    "#     sec_len = max_len\n",
    "#     max_idx = i\n",
    "#     max_len = curr_len\n",
    "#     max_sen = sentence1_test_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_telephone)\n",
    "test_acc_telephone = test_model_gru(test_loader, model_gru)\n",
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_slate)\n",
    "test_acc_slate = test_model_gru(test_loader, model_gru)\n",
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_government)\n",
    "test_acc_government = test_model_gru(test_loader, model_gru)\n",
    "MAX_SENTENCE_LENGTH, test_loader = preprocess_test_data(test_data_travel)\n",
    "test_acc_travel = test_model_gru(test_loader, model_gru)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multiplication error\n",
    "cat_train_loss_rnn = pkl.load(open('hidden_size_train_loss.p', 'rb'))\n",
    "cat_val_acc_rnn = pkl.load(open('hidden_size_val_acc.p', 'rb'))\n",
    "#mul_train_loss_rnn = pkl.load(open('mul_size_train_loss_rnn.p', 'rb'))\n",
    "#mul_val_acc_rnn = pkl.load(open('mul_size_val_acc_rnn.p', 'rb'))\n",
    "#add_train_loss_rnn = pkl.load(open('add_size_train_loss_rnn.p', 'rb'))\n",
    "#add_val_acc_rnn = pkl.load(open('add_size_val_acc_rnn.p', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "hidden_size_list = [200,400,600,800,1000]\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 5, figsize = (30,8))\n",
    "\n",
    "for i in range(5):\n",
    "    ax[0, i].plot(cat_train_loss_rnn[i])\n",
    "    ax[1, i].plot(cat_val_acc_rnn[i])\n",
    "    ax[0,i].set_xlabel(\"Training data size\")\n",
    "    ax[1,i].set_xlabel(\"Epochs\")\n",
    "    ax[0,i].set_ylabel(\"Training loss\")\n",
    "    ax[1,i].set_ylabel(\"Evaluation accuracy\")\n",
    "    ax[0,i].set_title(\"Hidden Size: {}\".format(hidden_size_list[i]))\n",
    "\n",
    "fig.savefig('cat_hidden_size_rnn_plot.png')\n",
    "files.download('cat_hidden_size_rnn_plot.png')\n",
    "\n",
    "# fig.savefig('mul_hidden_size_rnn_plot.png')\n",
    "# files.download('mul_hidden_size_rnn_plot.png')\n",
    "\n",
    "\n",
    "\n",
    "# fig.savefig('add_hidden_size_rnn_plot.png')\n",
    "# files.download('add_hidden_size_rnn_plot.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the examples by using non-shuffled validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_dataset = SnliDataset(sentence1_val_id, sentence2_val_id, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                         batch_size = BATCH_SIZE,\n",
    "                                         collate_fn = vocab_collate_func,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_gru_extract(loader, model):\n",
    "    model.eval()\n",
    "    for i, (data_1, length_1, data_2, length_2, labels) in enumerate(loader):\n",
    "        if i == 1:\n",
    "            break\n",
    "      \n",
    "    data_1_batch, length_1_batch, data_2_batch, length_2_batch,labels_batch = data_1, length_1, data_2, length_2, labels\n",
    "    data_1_batch = data_1_batch.cuda()\n",
    "    data_2_batch = data_2_batch.cuda()\n",
    "    length_1_batch = length_1_batch.cuda()\n",
    "    length_2_batch = length_2_batch.cuda()\n",
    "    labels_batch = labels_batch.cuda()\n",
    "\n",
    "    outputs = model(data_1_batch, data_2_batch, length_1_batch, length_2_batch).cuda()\n",
    "\n",
    "    outputs = F.softmax(outputs, dim = 1)\n",
    "\n",
    "    predicted = outputs.max(1, keepdim = True)[1].cuda()\n",
    "\n",
    "    result = predicted.eq(labels.view_as(predicted))\n",
    "\n",
    "    return data_1, data_2, labels, predicted, result\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Manual check for correct and incorrect classifications.\n",
    "data1_e, data2_2, labels_e, predicted_e, result_e = test_model_gru_extract(val_loader, model_gru_mul)\n",
    "data1_e1 = data1_e.cpu()\n",
    "data1_e1.numpy()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence1_val_data[37], sentence2_val_data[37], val_target[37]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
